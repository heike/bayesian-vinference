\documentclass[12pt]{article}
% \usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
% \geometry{letterpaper}                   % ... or a4paper or a5paper or ...
%\usepackage{graphicx}
\usepackage[font=small,skip=5pt]{caption}
\usepackage{subcaption}
\usepackage{afterpage}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amsfonts}
% \usepackage{color}
\usepackage{multirow}
\usepackage{rotating}
\usepackage[dvipsnames,svgnames,table]{xcolor}
\usepackage{hyperref}
\graphicspath{{figure/}}
% \usepackage{endfloat} % Figures to the end of the document

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
%---------------------------------------------------
%                 Editing Commands
\newcommand{\hh}[1]{{\color{magenta} #1}}
\newcommand{\srv}[1]{{\color{orange} #1}}
\newcommand{\eh}[1]{{\color{cyan} #1}}

%---------------------------------------------------
%                 Placing Figures


%---------------------------------------------------
% Define new environment
\newtheorem{theorem}{Theorem}[section]
\newtheorem{algorithm}[theorem]{Algorithm}
%---------------------------------------------------

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%


\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf A Bayesian approach to visual inference}
  \author{Susan VanderPlas, Eric Hare\thanks{
    The authors gratefully acknowledge funding from the National Science Foundation Grant \# DMS 1007697. All data collection has been conducted with approval from the Institutional Review Board IRB 10-347}\hspace{.2cm}\\
    Department of Statistics and Statistical Laboratory, Iowa State University\\
    and \\
    Heike Hofmann\\
    Department of Statistics and Statistical Laboratory, Iowa State University}
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf A Bayesian approach to visual inference}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
Graphics play a crucial role in statistical analysis and data mining. Being able to quantify structure in data that is visible in plots, and how people read the structure from plots is an ongoing challenge. \hh{XXX just a placeholder}
\end{abstract}

\noindent%
{\it Keywords:}  Visual inference, Lineup protocol, \hh{XXX Other keywords?}.
\vfill

\newpage
\spacingset{1.45} % DON'T change the spacing!

\tableofcontents
\newpage

<<setup, echo = FALSE, message = FALSE, warning = FALSE>>=
options(replace.assign=TRUE,width=70, digits=2)
require(knitr)
opts_chunk$set(fig.path='figure/', cache.path='cache/', fig.align='center', fig.width=5, fig.height=5, par=TRUE, cache=TRUE, concordance=TRUE, autodep=TRUE, message=F, warning=F)

library(dplyr)
@

\section{Introduction} 
Graphics are an important component of big data analysis, providing a mechanism for discovering unexpected patterns in data. Pioneering research by \citet{gelman:2004}, \citet{buja:2009} and \citet{majumder:2011} provide methods to quantify the significance of discoveries made from visualizations. %Although, there have been major advances in statistical graphics over the years, for example, systems like R \citep{R} provide high quality static graphics, and very recently some access to interactive graphics. But the problem remains that graphics are not widely considered to be a part of inferential statistics. 
\citet{buja:2009} introduced two protocols, the Rorschach and the lineup protocol, which bridge the gulf between traditional statistical inference and exploratory data analysis. The Rorschach protocol consists of a set of $m$ (usually, $m=20$) plots (called the {\it null plots}) rendered from data that is consistent with a given null model. That way, the Rorschach protocol helps to understand the extent of randomness in the null model. Under the lineup protocol, a plot of the observed data is placed randomly among a set of $m-1$ null plots.
Human observers are then asked to  examine the lineup and to identify the most different plot. If observers identify the data plot, this is quantifiable evidence against the null hypothesis. 
The lineup protocol places a statistical plot firmly in the framework of hypothesis tests: a plot of the data is considered to be the test statistic, which is compared against the sampling distribution under the null hypothesis represented by the null plots. 
Obviously, the null generating mechanism, i.e.\ the method of obtaining the data for null plots, is crucial for both the lineup and the Rorschach protocol. 
The null hypothesis directly affects the choice of null generating method. 
Null generating methods are typically based on (a) simulation, if the null hypothesis allows us to directly specify a parametric model, (b) sampling, as for example in the case of large data sets, or (c) permutation of the original data \citep[see e.g.\ ][]{Good05}, which allows for non-parametric testing  that preserves marginal distributions  while ensuring independence in higher dimensions. 
%In the experimental data that we analyzed the null generating methods used were permutation methods and direct simulation from a null model.

The lineup protocol was formally tested in a head-to-head comparison with the equivalent conventional test in \citet{majumder:2011}. The experiment utilized human subjects from Amazon's Mechanical Turk \citep{turk} and used simulation to control conditions. The results suggest that  visual inference is comparable to conventional tests in a controlled conventional setting. This provides support for its appropriateness for testing in real exploratory situations where no conventional test exists. Interestingly, the power of a visual test increases with the number of observers engaged to evaluate lineups, and the pattern in results suggests that the power will provide results consistent with practical significance \citep{kirk:1996}.

\section{Rough thoughts on the model}
Let $p_i$, $i = 1, ..., m$ be the probability that plot $i$ is picked. We know that $0 \le p_i \le 1$ with $\sum_{i=1}^m p_i = 1$.
We can assign an uninformative prior probability to $p$ by using a flat Dirichlet distribution (a uniform distribution over the $(m-1)$ simplex).

We will use the following model:
\begin{eqnarray*}
\alpha &=& (\alpha_1, \alpha_2, ..., \alpha_m) = \text{concentration hyperparameter}\\
p \mid \alpha &=& (p_1, p_2, ..., p_m) \sim Dir (m, \alpha) \\
x \mid p &=& (x_1, x_2, ..., x_m) \sim Mult (m, p)
\end{eqnarray*}
The posterior distribution is then as follows:
\begin{eqnarray*}
c &=& (c_1, ..., c_m) = \text{ number of picks of each plot} \\
p \mid x, \alpha &\sim& Dir (K, \alpha_1+c_1, \alpha_2+c_2, ..., \alpha_m + c_m)
\end{eqnarray*}

For visual inference, we are usually dealing with a null hypothesis of the form `the plot of the data is not in any way different from the null plots'.  Under this null hypothesis the vector of concentration parameters $\alpha$ has to consist of the same values, i.e.\ $\alpha_i = $const. The sum of the concentration parameters $\sum{\alpha}$ can be interpreted as the number of (pseudo)evaluations  under the null hypothesis. Jeffreys' uninformative prior \citep{jeffreys:1946} for this situation is $\alpha = \frac{1}{2}$, corresponding to 10 pseudo-evaluations.
\hh{XXX this is likely a parameter we have to fine-tune, right now it is set to be $\alpha = 1/2$. 
For $\alpha = 1$ we need a lot of data (in form of evaluations by observers) in order to get the posterior distribution away from the prior, even if we have a strong signal.}


Note that one of the advantages of this model is that it allows a seamless incorporation of results from multiple picks by using reciprocal weights: if an evaluator identified three panels as the `most different', each of these three panels gets an additional $1/3$ to its overall number of picks $c$. 

\begin{figure}
\centering
<<prior, echo=FALSE, fig.width = 8, fig.height = 8, out.width='0.7\\textwidth'>>=
library(gtools)
library(reshape2)
library(ggplot2)

m <- 20
alpha <- rep(1/2, m)
lps <- data.frame(rdirichlet(5000, alpha))

lpsm <- melt(lps, measure.var=1:20)
lpsm$variable <- as.numeric(gsub("X", "", lpsm$variable))

qplot(value, geom="density", data=lpsm, fill=I("grey50"), 
      colour=I("grey45"), alpha=I(0.5)) + 
  facet_wrap(~variable) + xlim(c(0,1)) +
  theme_bw() +
  xlab("Probability to pick plot")
@
\caption{\label{fig:prior}General prior for any lineup of size 20. }
\end{figure}

<<posterior, echo=FALSE, cache = TRUE, warning=FALSE, message=FALSE>>=
getCounts <- function(response) {
  results <- strsplit(response,split=",")
  m <- 20
  wt <- sapply(results, length)
  wt <- 1/rep(wt, wt)
  picks <- factor(as.numeric(unlist(results)), levels=1:20)

  counts <- xtabs(wt~picks)
  as.vector(counts)
}


bfactor <- function(alpha, counts) {
  x <- seq(0,0.9999, by=0.01)
  bf <- lapply(1:20, function(i) {
    pre <- pbeta(x, shape1=alpha[i], shape2=sum(alpha)-alpha[i], 
                           lower.tail=FALSE)
    post <- pbeta(x, shape1=alpha[i]+counts[i], 
                shape2=sum(alpha)-alpha[i] + sum(counts) - counts[i],
                lower.tail=FALSE)
    post/pre
  })
  # now get averages
  bfavg <- bf %>% lapply(sum) %>% unlist
  bfmode <- bf %>% lapply(max) %>% unlist
  
  list(avg = bfavg/length(x), mode = bfmode, factor = bf)
}

pre_post_picks <- function(data) {
  # needs to have a picture id
  data.picks <- data %>% group_by(pic_id) %>% do(
    counts = getCounts(.$response_no))
  data.picks <- data.picks %>% group_by(pic_id) %>% mutate(
    picks = sum(counts[[1]]),
    post_mean = list((alpha + counts[[1]])/sum(alpha + counts[[1]])),
    post_mode = list((alpha + counts[[1]] - 1)/sum(alpha + counts[[1]]-1)),
    bfactor = list(bfactor(alpha, counts[[1]])),
    pre_prob  = list(pbeta(1/20*2, shape1=alpha, shape2=sum(alpha)-alpha, 
                           lower.tail=FALSE)),
    post_prob = list(pbeta(1/20*2, shape1=alpha+counts[[1]], 
                           shape2=sum(alpha)-alpha + sum(counts[[1]]) - counts[[1]], 
                           lower.tail=FALSE))
  )
  data.picks
}

turk19 <- read.csv("data/turk19_results_anon.csv", stringsAsFactors = FALSE)
turk16 <- read.csv("data/turk16_results.csv", stringsAsFactors = FALSE)
turk1013 <- read.csv("data/turk1013_results.csv", stringsAsFactors = FALSE)

pics19 <- read.csv("data/picture-details-turk19.csv", stringsAsFactors = FALSE)
pics16 <- read.csv("data/picture-details-turk16.csv", stringsAsFactors = FALSE)
pics10 <- read.csv("data/picture-details-turk10.csv", stringsAsFactors = FALSE)

turk19.picks <- pre_post_picks(turk19)
turk16.picks <- pre_post_picks(turk16)
turk1013.picks <- pre_post_picks(turk1013)
@

\begin{figure}
\begin{subfigure}[b]{.49\textwidth}
\caption{\label{fig:lp-104}Example lineup. }
\includegraphics[width=\textwidth]{lineup-images/filebab6558ba4c4-multiple.pdf}
\end{subfigure}
\begin{subfigure}[b]{.49\textwidth}
\caption{\label{fig:posterior}Posterior densities for the choosing panel \#$i$. }
<<pics-turk1013, dependson='posterior', echo=FALSE,  fig.width=8, fig.height = 8, out.width='\\textwidth'>>=
# show one of the results:
i <- 1

lpsXXX <- data.frame(rdirichlet(5000, alpha+turk1013.picks$counts[i][[1]]))
lpsXXXm <- melt(lpsXXX, measure.var=1:20)
lpsXXXm$variable <- as.numeric(gsub("X", "", lpsXXXm$variable))

counts <- turk1013.picks[i, "counts"][[1]][[1]]
#probs <- turk1013.picks[i, "post_prob"][[1]][[1]]
#preprobs <- turk1013.picks[i, "pre_prob"][[1]][[1]]
bfactor <- turk1013.picks[i, "bfactor"]$bfactor[[1]]$avg
dframe <- data.frame(variable = 1:20, Bfactor= round(bfactor,2))

ymax <- 1:20 %>% lapply(function(i) max(density(subset(lpsXXXm, variable == i)$value)$y)) %>% unlist %>% max

qplot(value, geom="density", data=lpsXXXm, fill=I("grey50"), 
      colour=I("grey45"), alpha=I(0.5)) + 
  facet_wrap(~variable) + xlim(c(0,1)) +
  theme_bw() +
  xlab("Posterior probability to pick plot") +
  ggtitle(sprintf("Pic ID %s (based on %.1f picks)", turk1013.picks$pic_id[i],
                  sum(turk1013.picks$counts[i][[1]]))) + 
  geom_text(data=dframe, aes(label = Bfactor), x = 1, y = 0.95*ymax, size = 5, 
            colour="grey70", hjust="inward", vjust="inward")
  
@
\end{subfigure}
\caption{\label{fig:xpl-turk1013} Lineup (left) and posterior densities (right) corresponding to evaluations by independent observers.}
\end{figure}


\begin{figure}
\begin{subfigure}[b]{.49\textwidth}
\caption{\label{fig:lp-104}Example lineup. }
\includegraphics[width=\textwidth]{lineup-images/34fcf946135adb1c03b147897f20b33d.pdf}
\end{subfigure}
\begin{subfigure}[b]{.49\textwidth}
\caption{\label{fig:posterior}Posterior densities for the choosing panel \#$i$. }
<<pics, dependson='posterior', echo=FALSE,  fig.width=8, fig.height = 8, out.width='\\textwidth'>>=

# show one of the results:
i <- 1

lpsXXX <- data.frame(rdirichlet(5000, alpha+turk19.picks$counts[i][[1]]))
lpsXXXm <- melt(lpsXXX, measure.var=1:20)
lpsXXXm$variable <- as.numeric(gsub("X", "", lpsXXXm$variable))

counts <- turk19.picks[i, "counts"][[1]][[1]]
probs <- turk19.picks[i, "post_prob"][[1]][[1]]
preprobs <- turk19.picks[i, "pre_prob"][[1]][[1]]
bfactor <- turk19.picks[i, "bfactor"]$bfactor[[1]]$avg
bfs <- turk19.picks[i, "bfactor"]$bfactor[[1]]$factor
dframe <- data.frame(variable = 1:20, Bfactor= round(bfactor,2))

ymax <- 1:20 %>% lapply(function(i) max(density(subset(lpsXXXm, variable == i)$value)$y)) %>% unlist %>% max

qplot(value, geom="density", data=lpsXXXm, fill=I("grey50"), 
      colour=I("grey45"), alpha=I(0.5)) + 
  facet_wrap(~variable) + xlim(c(0,1)) +
  theme_bw() +
  xlab("Posterior probability to pick plot") +
  ggtitle(sprintf("Pic ID %s (based on %.1f picks)", turk19.picks$pic_id[i],
                  sum(turk19.picks$counts[i][[1]]))) + 
  geom_text(data=dframe, aes(label = Bfactor), x = 1, y = 0.9*ymax, size = 5, 
            colour="grey70", hjust="inward", vjust="inward")
  
@
\end{subfigure}
\caption{\label{fig:xpl-104} Lineup (left) and posterior densities (right) corresponding to evaluations by independent observers.}
\end{figure}

For each individual panel of the lineup, the marginal distributions are Beta distributions. For the distribution of picking panel $i$ the prior distribution is therefore a Beta distribution Beta$(1, 19)$. In the example of Figure~\ref{fig:posterior} we have a Beta distribution of Beta$(\Sexpr{counts[11]+ alpha[11]}, \Sexpr{sum(alpha)+sum(counts)-counts[11]-alpha[11]})$ for panel \#11 and Beta$(\Sexpr{counts[20]+ alpha[20]}, \Sexpr{sum(alpha)+sum(counts)-counts[20]-alpha[20]})$ for panel \#20. 


\paragraph{Calculation of the Bayes factor for plot \#$i$}\hfill\newline
\hh{XXX after discussion with Susan: put a prior uniform on the cutoff threshold $c$ - either integrate it out or find a mode in the posterior.}
\hh{XXX  integration seems to work out really well - what do you think? }

In order to assess the strength of the signal of plot \#$i$, we assess the probability that an observer chooses this plot at a pre-specified level $c$ and compare the posterior probability to its prior for a Bayes factor. 
More formally, we define the Bayes factor of choosing plot \#$i$ given threshold level $c$ as 
\[
B_c(p_i) = \frac{P(p_i > c \mid x, \alpha)}{P(p_i > c \mid \alpha)},
\]
where $c$ is a pre-specified threshold with $c \in (0,1)$ and $p_i$ is the probability that an observer chooses plot \#$i$, $1 \le i \le m$, as the most different. 
The choice of $c$ is critical: a natural choice would be $c = 1/m$, where $m$ is the size of the lineup. However, the prior probability in a lineup of size 20 is $\Sexpr{round(pbeta(1/20, shape1=alpha[1], shape2=sum(alpha)-alpha[1], lower.tail=FALSE), 3)}$. This puts an upper limit on the Bayes factor of $\Sexpr{round(1/pbeta(1/20, shape1=alpha[1], shape2=sum(alpha)-alpha[1], lower.tail=FALSE), 2)}$ (the reciprocal value of the prior). A higher threshold allows for a higher upper limit on the Bayes factor, e.g.\ a threshold of $c = \frac{2}{m}$ already allows for a Bayes factor of up to \Sexpr{round(1/pbeta(1/20*2, shape1=alpha[1], shape2=sum(alpha)-alpha[1], lower.tail=FALSE),2)}. For the example shown in Figure~\ref{fig:xpl-104}, the posterior probabilities of picking panels \#11 and \#20 with a probability of at least $c = \frac{2}{m}$ are \Sexpr{round(probs[11],4)} and \Sexpr{round(probs[20],4)}, respectively. This corresponds to Bayes factors of  \Sexpr{round(probs[11]/preprobs[11],2)} and \Sexpr{round(probs[20]/preprobs[20],2)}.
An overview of the Bayes factors $B_c$ for a specific lineup is shown in Figure~\ref{fig:bc-104}.

\begin{figure}
\begin{subfigure}[b]{.49\textwidth}
\caption{\label{fig:lp-104-bc}Example lineup. }
\includegraphics[width=\textwidth]{lineup-images/34fcf946135adb1c03b147897f20b33d.pdf}
\end{subfigure}
\begin{subfigure}[b]{.49\textwidth}
\caption{\label{fig:bfactor}Bayes factors at each value of $c \in (0,1)$. }
<<Bfactorc, echo=FALSE, fig.width=7, fig.height=7>>=
x <- seq(0,0.9999, by=0.01)
BFS <- 1:20 %>% lapply(function(i) data.frame(i, x = x, bf=bfs[[i]])) %>% bind_rows
BFS$capbf <- pmin(BFS$bf, 20)
qplot(x, capbf,  geom="line", data= BFS) + theme_bw() + 
  facet_wrap(facets = ~i) + 
  ylab("Bayes factor at threshold c") + 
  xlab("Threshold c") #+ ylim(c(0,20))
@
\end{subfigure}
\caption{\label{fig:bc-104} Lineup (left) and Bayes factors at threshold $c$ (right). Note that the Bayes factors for plot \#20 are capped at a value of 20. Uncapped, the Bayes factor for \#20 reaches a mode of over 300. }
\end{figure}

Instead of deciding for a single value of $c$, 
we integrate over all possible values of $c$ and get the Bayes factor for plot \#$i$ as:
\[
B(p_i) = \int_0^1 B_c(p_i) dc = \int_0^1 \frac{P(p_i > c \mid x, \alpha)}{P(p_i > c \mid \alpha)} dc.
\]

There are different schemes interpreting the Bayes factor. \citet{jeffreys:1961} consider a factor of 5 to 10 as `substantial', and values above 20 as `decisive'. 
Alternatively, \citet{kass:1995} consider a factor of 6 to 10 as `strong', and values above 10 as `very strong'.

\hh{XXX investigate Bayes factor of different scenarios, ie ten evaluations, different number of data picks and null picks. How does that compare to the visual distribution $V_3$? (the Binomial like distribution that is adjusted for dependencies between the data and the nulls). }

\begin{figure}
\begin{subfigure}[b]{.49\textwidth}
\caption{\label{fig:lp-114}Example lineup. }
\includegraphics[width=\textwidth]{lineup-images/e30ff06449a4b7664fe3109f7e2e996f.pdf}
\end{subfigure}
\begin{subfigure}[b]{.49\textwidth}
\caption{\label{fig:posterior2}Posterior densities for the choosing panel \#$i$. }
<<pics2, dependson='posterior', echo=FALSE,  fig.width=8, fig.height = 8, out.width='\\textwidth'>>=

# show one of the results:
i <- 2

lpsXXX <- data.frame(rdirichlet(5000, alpha+turk19.picks$counts[i][[1]]))
lpsXXXm <- melt(lpsXXX, measure.var=1:20)
lpsXXXm$variable <- as.numeric(gsub("X", "", lpsXXXm$variable))

counts <- turk19.picks[i, "counts"][[1]][[1]]
#probs <- turk19.picks[i, "post_prob"][[1]][[1]]
#preprobs <- turk19.picks[i, "pre_prob"][[1]][[1]]
bfactor <- turk19.picks[i, "bfactor"]$bfactor[[1]]$avg
dframe <- data.frame(variable = 1:20, Bfactor= round(bfactor,2))

ymax <- 1:20 %>% lapply(function(i) max(density(subset(lpsXXXm, variable == i)$value)$y)) %>% unlist %>% max

qplot(value, geom="density", data=lpsXXXm, fill=I("grey50"), 
      colour=I("grey45"), alpha=I(0.5)) + 
  facet_wrap(~variable) + xlim(c(0,1)) +
  theme_bw() +
  xlab("Posterior probability to pick plot") +
  ggtitle(sprintf("Pic ID %s (based on %.1f picks)", turk19.picks$pic_id[i],
                  sum(turk19.picks$counts[i][[1]]))) + 
  geom_text(data=dframe, aes(label = Bfactor), x = 1, y = 0.9*ymax, size = 5, 
            colour="grey70", hjust="inward", vjust="inward")
@
\end{subfigure}
\caption{\label{fig:xpl-114} Lineup (left) and posterior densities (right) corresponding to evaluations by independent observers.}
\end{figure}

\begin{figure}
\begin{subfigure}[b]{.49\textwidth}
\caption{\label{fig:lp-114}Example lineup. }
\includegraphics[width=\textwidth]{lineup-images/5f9885168e3a02f57ab7216e9f76141d.pdf}
\end{subfigure}
\begin{subfigure}[b]{.49\textwidth}
\caption{\label{fig:posterior2}Posterior densities for the choosing panel \#$i$. }
<<pics3, dependson='posterior', echo=FALSE,  fig.width=8, fig.height = 8, out.width='\\textwidth'>>=
dname <- gsub("-turk16","", subset(pics19, pic_id == 104)$data_name)
dname <- gsub("sd\\.","sd", dname)
dname <- gsub("trend", "line", dname)

samedata <- subset(pics16, data_name == dname)
# subset(samedata, test_param == "turk16-colorEllipse")
# identifies 5f9885168e3a02f57ab7216e9f76141d

# show one of the results:
i <- subset(samedata, test_param == "turk16-colorEllipse")$pic_id

lpsXXX <- data.frame(rdirichlet(5000, alpha+turk16.picks$counts[i][[1]]))
lpsXXXm <- melt(lpsXXX, measure.var=1:20)
lpsXXXm$variable <- as.numeric(gsub("X", "", lpsXXXm$variable))

counts <- turk16.picks[i, "counts"][[1]][[1]]
#probs <- turk16.picks[i, "post_prob"][[1]][[1]]
#preprobs <- turk16.picks[i, "pre_prob"][[1]][[1]]
bfactor <- turk16.picks[i, "bfactor"]$bfactor[[1]]$avg
dframe <- data.frame(variable = 1:20, Bfactor= round(bfactor,2))

ymax <- 1:20 %>% lapply(function(i) max(density(subset(lpsXXXm, variable == i)$value)$y)) %>% unlist %>% max

qplot(value, geom="density", data=lpsXXXm, fill=I("grey50"), 
      colour=I("grey45"), alpha=I(0.5)) + 
  facet_wrap(~variable) + xlim(c(0,1)) +
  theme_bw() +
  xlab("Posterior probability to pick plot") +
  ggtitle(sprintf("Pic ID %s (based on %.1f picks)", turk16.picks$pic_id[i],
                  sum(turk16.picks$counts[i][[1]]))) + 
  geom_text(data=dframe, aes(label = Bfactor), x = 1, y = 0.9*ymax, size = 5, 
            colour="grey70", hjust="inward", vjust="inward")

@
\end{subfigure}
\caption{\label{fig:xpl-124} Lineup (left) and posterior densities (right) corresponding to evaluations by independent observers.}
\end{figure}

\bibliographystyle{asa}
\bibliography{references}

\end{document}
