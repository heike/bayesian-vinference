\documentclass[12pt]{article}
\usepackage{natbib}
% %\usepackage{fullpage}
\usepackage{color}
\usepackage[dvipsnames,svgnames*]{xcolor}
\usepackage{array}
\usepackage[colorlinks=TRUE, linkcolor=blue]{hyperref}
\usepackage{wrapfig,float}
\usepackage[font=small,skip=5pt]{caption}
\usepackage{subcaption}
\usepackage{graphicx}
%\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{ulem}
\usepackage[section]{placeins}
\usepackage{afterpage}

\graphicspath{{figure/}}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\newcommand{\hh}[1]{{\color{magenta} #1}}
\newcommand{\srv}[1]{{\color{orange} #1}}
\newcommand{\eh}[1]{{\color{cyan} #1}}


%\usepackage[dvips]{graphics}
\newtheorem{thm}{Theorem}[section]
\newtheorem{dfn}{Definition}[section]
\newtheorem{cor}{Corollary}[thm]
\newtheorem{con}{Conjecture}[thm]
%\setlength{\parindent}{0in}   % for no indent

%\topmargin -0.10in   % when making pdf
%\textheight 9.15in  % when making pdf

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%


\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf A Bayesian approach to visual inference}
\author{Susan Vander Plas, Eric Hare\thanks{
    The authors gratefully acknowledge funding from the National Science Foundation Grant DMS \#1007697. All data collection has been conducted with approval from the Institutional Review Board IRB 10-347.}\hspace{.2cm}\\
    Department of Statistics and Statistical Laboratory, Iowa State University\\
    and \\
    Heike Hofmann\\
    Department of Statistics and Statistical Laboratory, Iowa State University \\
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf A Bayesian approach to visual inference}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
Graphics play a crucial role in statistical analysis and data mining. Being able to quantify structure in data that is visible in plots, and how people read the structure from plots is an ongoing challenge. 
\end{abstract}

\noindent%
{\it Keywords:}  data visualization, statistical graphics, data mining, data science, information visualization
\vfill

\newpage
\spacingset{1.45} % DON'T change the spacing!
\tableofcontents
\newpage


<<setup, echo=FALSE, results='hide', message=FALSE, warning=FALSE>>=
library(reshape2)
suppressMessages(library(ggplot2))
library(grid)
library(nullabor)
library(dplyr)
theme_lineup <- theme(axis.text = element_blank(), 
                      axis.title = element_blank(),
                      axis.ticks = element_blank(),
                      plot.margin=unit(c(0,0,0,0), unit="cm"))
@

\section{Introduction} 
Graphics are an important component of big data analysis, providing a mechanism for discovering unexpected patterns in data. Pioneering research by \citet{gelman:2004}, \citet{buja:2009} and \citet{majumder:2011} provide methods to quantify the significance of discoveries made from visualizations. %Although, there have been major advances in statistical graphics over the years, for example, systems like R \citep{R} provide high quality static graphics, and very recently some access to interactive graphics. But the problem remains that graphics are not widely considered to be a part of inferential statistics. 
\citet{buja:2009} introduced two protocols, the Rorschach and the lineup protocol, which bridge the gulf between traditional statistical inference and exploratory data analysis. The Rorschach protocol consists of a set of $m$ (usually, $m=20$) plots (called the {\it null plots}) rendered from data that is consistent with a given null model. That way, the Rorschach protocol helps to understand the extent of randomness in the null model. Under the lineup protocol, a plot of the observed data is placed randomly among a set of $m-1$ null plots.
Human observers are then asked to  examine the lineup and to identify the most different plot. If observers identify the data plot, this is quantifiable evidence against the null hypothesis. 
The lineup protocol places a statistical plot firmly in the framework of hypothesis tests: a plot of the data is considered to be the test statistic, which is compared against the sampling distribution under the null hypothesis represented by the null plots. 
Obviously, the null generating mechanism, i.e.\ the method of obtaining the data for null plots, is crucial for both the lineup and the Rorschach protocol. 
The null hypothesis directly affects the choice of null generating method. 
Null generating methods are typically based on (a) simulation, if the null hypothesis allows us to directly specify a parametric model, (b) sampling, as for example in the case of large data sets, or (c) permutation of the original data \citep[see e.g.\ ][]{Good05}, which allows for non-parametric testing  that preserves marginal distributions  while ensuring independence in higher dimensions. 
%In the experimental data that we analyzed the null generating methods used were permutation methods and direct simulation from a null model.

The lineup protocol was formally tested in a head-to-head comparison with the equivalent conventional test in \citet{majumder:2011}. The experiment utilized human subjects from Amazon's Mechanical Turk \citep{turk} and used simulation to control conditions. The results suggest that  visual inference is comparable to conventional tests in a controlled conventional setting. This provides support for its appropriateness for testing in real exploratory situations where no conventional test exists. Interestingly, the power of a visual test increases with the number of observers engaged to evaluate lineups, and the pattern in results suggests that the power will provide results consistent with practical significance \citep{kirk:1996}.

\section{Rough thoughts on the model}
Let $p_i$, $i = 1, ..., m$ be the probability that plot $i$ is picked. We know that $0 \le p_i \le 1$ with $\sum_{i=1}^m p_i = 1$.
We can assign an uninformative prior probability to $p$ by using a flat Dirichlet distribution (a uniform distribution over the $(m-1)$ simplex).

We will use the following model:
\begin{eqnarray*}
\alpha &=& (\alpha_1, \alpha_2, ..., \alpha_m) = \text{concentration hyperparameter}\\
p \mid \alpha &=& (p_1, p_2, ..., p_m) \sim Dir (m, \alpha) \\
x \mid p &=& (x_1, x_2, ..., x_m) \sim Mult (m, p)
\end{eqnarray*}
The posterior distributions are then as follows:
\begin{eqnarray*}
c &=& (c_1, ..., c_m) = \text{ number of picks of each plot} \\
p \mid x, \alpha &\sim& Dir (K, \alpha_1+c_1, \alpha_2+c_2, ..., \alpha_m + c_m)
\end{eqnarray*}

For visual inference, we are usually dealing with a null hypothesis of the form `the plot of the data is not in any way different from the null plots'.  Under this null hypothesis the vector of concentration parameters $\alpha$ has to consist of the same values, i.e.\ $\alpha_i = $const. The sum of the concentration parameters $\sum{\alpha}$ can be interpreted as the number of evaluations we allow under the null hypothesis. Jeffrey's uninformative prior \citep{jeffreys:1946} for this situation is $\alpha = \frac{1}{2}$.
\hh{XXX this is likely a parameter we have to fine-tune, right now it is set to be $\alpha = 1/2$. 
For $\alpha = 1$ we need a lot of data (in form of evaluations by observers) in order to get the posterior distribution away from the prior, even if we have a strong signal.}

\begin{figure}
\centering
<<prior, echo=FALSE, fig.width = 8, fig.height = 8, out.width='0.7\\textwidth'>>=
library(gtools)
library(reshape2)
library(ggplot2)

m <- 20
alpha <- rep(1/2, m)
lps <- data.frame(rdirichlet(5000, alpha))

lpsm <- melt(lps, measure.var=1:20)
lpsm$variable <- as.numeric(gsub("X", "", lpsm$variable))

qplot(value, geom="density", data=lpsm, fill=I("grey50"), 
      colour=I("grey45"), alpha=I(0.5)) + 
  facet_wrap(~variable) + xlim(c(0,1)) +
  theme_bw() +
  xlab("Probability to pick plot")
@
\caption{\label{fig:prior}General prior for any lineup of size 20. }
\end{figure}

<<posterior, echo=FALSE, warning=FALSE, message=FALSE>>=
library(dplyr)
turk19 <- read.csv("data/turk19_results_anon.csv", stringsAsFactors = FALSE)
turk16 <- read.csv("data/turk16_results.csv", stringsAsFactors = FALSE)
pics19 <- read.csv("data/picture-details-turk19.csv", stringsAsFactors = FALSE)
pics16 <- read.csv("data/picture-details-turk16.csv", stringsAsFactors = FALSE)

getCounts <- function(response) {
  results <- strsplit(response,split=",")
  m <- 20
  wt <- sapply(results, length)
  wt <- 1/rep(wt, wt)
  picks <- factor(as.numeric(unlist(results)), levels=1:20)

  counts <- xtabs(wt~picks)
  as.vector(counts)
}
turk19.picks <- turk19 %>% group_by(pic_id) %>% do(
  counts = getCounts(.$response_no))
turk19.picks <- turk19.picks %>% group_by(pic_id) %>% mutate(
  picks = sum(counts[[1]]),
  post_mean = list((alpha + counts[[1]])/sum(alpha + counts[[1]])),
  post_mode = list((alpha + counts[[1]] - 1)/sum(alpha + counts[[1]]-1)),
  pre_prob  = list(pbeta(1/20*2, shape1=alpha, shape2=sum(alpha)-alpha, 
                    lower.tail=FALSE)),
  post_prob = list(pbeta(1/20*2, shape1=alpha+counts[[1]], 
                    shape2=sum(alpha)-alpha + sum(counts[[1]]) - counts[[1]], 
                    lower.tail=FALSE))
  )


turk16.picks <- turk16 %>% group_by(pic_id) %>% do(
  counts = getCounts(.$response_no))
turk16.picks <- turk16.picks %>% group_by(pic_id) %>% mutate(
  picks = sum(counts[[1]]),
  post_mean = list((alpha + counts[[1]])/sum(alpha + counts[[1]])),
  post_mode = list((alpha + counts[[1]] - 1)/sum(alpha + counts[[1]]-1)),
  pre_prob  = list(pbeta(1/20*2, shape1=alpha, shape2=sum(alpha)-alpha, 
                    lower.tail=FALSE)),
  post_prob = list(pbeta(1/20*2, shape1=alpha+counts[[1]], 
                    shape2=sum(alpha)-alpha + sum(counts[[1]]) - counts[[1]], 
                    lower.tail=FALSE))
  )
@
\begin{figure}
\begin{subfigure}[b]{.49\textwidth}
\caption{\label{fig:lp-104}Example lineup. }
\includegraphics[width=\textwidth]{lineup-images/34fcf946135adb1c03b147897f20b33d.pdf}
\end{subfigure}
\begin{subfigure}[b]{.49\textwidth}
\caption{\label{fig:posterior}Posterior densities for one specific lineup. }
<<pics, dependson='posterior', echo=FALSE,  fig.width=8, fig.height = 8, out.width='\\textwidth'>>=

# show one of the results:
i <- 1

lpsXXX <- data.frame(rdirichlet(5000, alpha+turk19.picks$counts[i][[1]]))
lpsXXXm <- melt(lpsXXX, measure.var=1:20)
lpsXXXm$variable <- as.numeric(gsub("X", "", lpsXXXm$variable))

counts <- turk19.picks[i, "counts"][[1]][[1]]
probs <- turk19.picks[i, "post_prob"][[1]][[1]]
preprobs <- turk19.picks[i, "pre_prob"][[1]][[1]]
dframe <- data.frame(variable = 1:20, Bfactor=round(probs/preprobs,2))

ymax <- 1:20 %>% lapply(function(i) max(density(subset(lpsXXXm, variable == i)$value)$y)) %>% unlist %>% max

qplot(value, geom="density", data=lpsXXXm, fill=I("grey50"), 
      colour=I("grey45"), alpha=I(0.5)) + 
  facet_wrap(~variable) + xlim(c(0,1)) +
  theme_bw() +
  xlab("Posterior probability to pick plot") +
  ggtitle(sprintf("Pic ID %s (based on %.1f picks)", turk19.picks$pic_id[i],
                  sum(turk19.picks$counts[i][[1]]))) + 
  geom_text(data=dframe, aes(label = Bfactor), x = 1, y = 0.9*ymax, size = 5, 
            colour="grey70", hjust="inward", vjust="inward")
  
@
\end{subfigure}
\caption{\label{fig:xpl-104} Lineup (left) and posterior densities (right) corresponding to evaluations by independent observers.}
\end{figure}

For each individual panel of the lineup, the marginal distributions are Beta distributions. For the distribution of picking panel $i$ the prior distribution is therefore a Beta distribution Beta$(1, 19)$. In the example of Figure~\ref{fig:posterior} we have a Beta distribution of Beta$(\Sexpr{counts[11]+ alpha[11]}, \Sexpr{sum(alpha)+sum(counts)-counts[11]-alpha[11]})$ for panel \#11 and Beta$(\Sexpr{counts[20]+ alpha[20]}, \Sexpr{sum(alpha)+sum(counts)-counts[20]-alpha[20]})$ for panel \#20. 

\hh{XXX knitr questionL how do I set the number of digits in the output automatically?}
\hh{XXX stats question: next step to work out: get a Bayes factor of picking the data plot posteriori compared to a-priori. Should we use the probability $P(p > \frac{1}{m})$?  The prior probability in lineup of size 20 is $\Sexpr{pbeta(1/20, shape1=alpha[1], shape2=sum(alpha)-alpha[1], lower.tail=FALSE)}$, so we might need to get to a higher probability to show a difference:
$P(p > 2 \frac{1}{m}) = $\Sexpr{pbeta(1/20*2, shape1=alpha[1], shape2=sum(alpha)-alpha[1], lower.tail=FALSE)}. 
For the example here, the posterior probabilities of $P(p > 2 \frac{1}{m})$ are \Sexpr{probs[11]} and \Sexpr{probs[20]} for panels \#11 and \#20, respectively. This corresponds to a Bayes factor of  \Sexpr{probs[11]/preprobs[11]} and \Sexpr{probs[20]/preprobs[20]}.
}

\hh{XXX what is nice about this model is that it seamlessly allows to incorporate results from multiple picks by using reciprocal weights: if an evaluator identified three panels as the `most different', each of these three panels gets an additional $1/3$ to its overall count.}

\begin{figure}
\begin{subfigure}[b]{.49\textwidth}
\caption{\label{fig:lp-114}Example lineup. }
\includegraphics[width=\textwidth]{lineup-images/e30ff06449a4b7664fe3109f7e2e996f.pdf}
\end{subfigure}
\begin{subfigure}[b]{.49\textwidth}
\caption{\label{fig:posterior2}Posterior densities for one specific lineup. }
<<pics2, dependson='posterior', echo=FALSE,  fig.width=8, fig.height = 8, out.width='\\textwidth'>>=

# show one of the results:
i <- 2

lpsXXX <- data.frame(rdirichlet(5000, alpha+turk19.picks$counts[i][[1]]))
lpsXXXm <- melt(lpsXXX, measure.var=1:20)
lpsXXXm$variable <- as.numeric(gsub("X", "", lpsXXXm$variable))

counts <- turk19.picks[i, "counts"][[1]][[1]]
probs <- turk19.picks[i, "post_prob"][[1]][[1]]
preprobs <- turk19.picks[i, "pre_prob"][[1]][[1]]
dframe <- data.frame(variable = 1:20, Bfactor=round(probs/preprobs,2))

ymax <- 1:20 %>% lapply(function(i) max(density(subset(lpsXXXm, variable == i)$value)$y)) %>% unlist %>% max

qplot(value, geom="density", data=lpsXXXm, fill=I("grey50"), 
      colour=I("grey45"), alpha=I(0.5)) + 
  facet_wrap(~variable) + xlim(c(0,1)) +
  theme_bw() +
  xlab("Posterior probability to pick plot") +
  ggtitle(sprintf("Pic ID %s (based on %.1f picks)", turk19.picks$pic_id[i],
                  sum(turk19.picks$counts[i][[1]]))) + 
  geom_text(data=dframe, aes(label = Bfactor), x = 1, y = 0.9*ymax, size = 5, 
            colour="grey70", hjust="inward", vjust="inward")
@
\end{subfigure}
\caption{\label{fig:xpl-114} Lineup (left) and posterior densities (right) corresponding to evaluations by independent observers.}
\end{figure}

\begin{figure}
\begin{subfigure}[b]{.49\textwidth}
\caption{\label{fig:lp-114}Example lineup. }
\includegraphics[width=\textwidth]{lineup-images/5f9885168e3a02f57ab7216e9f76141d.pdf}
\end{subfigure}
\begin{subfigure}[b]{.49\textwidth}
\caption{\label{fig:posterior2}Posterior densities for one specific lineup. }
<<pics3, dependson='posterior', echo=FALSE,  fig.width=8, fig.height = 8, out.width='\\textwidth'>>=
dname <- gsub("-turk16","", subset(pics19, pic_id == 104)$data_name)
dname <- gsub("sd\\.","sd", dname)
dname <- gsub("trend", "line", dname)

samedata <- subset(pics16, data_name == dname)
# subset(samedata, test_param == "turk16-colorEllipse")
# identifies 5f9885168e3a02f57ab7216e9f76141d

# show one of the results:
i <- subset(samedata, test_param == "turk16-colorEllipse")$pic_id

lpsXXX <- data.frame(rdirichlet(5000, alpha+turk16.picks$counts[i][[1]]))
lpsXXXm <- melt(lpsXXX, measure.var=1:20)
lpsXXXm$variable <- as.numeric(gsub("X", "", lpsXXXm$variable))

counts <- turk16.picks[i, "counts"][[1]][[1]]
probs <- turk16.picks[i, "post_prob"][[1]][[1]]
preprobs <- turk16.picks[i, "pre_prob"][[1]][[1]]
dframe <- data.frame(variable = 1:20, Bfactor=round(probs/preprobs,2))

ymax <- 1:20 %>% lapply(function(i) max(density(subset(lpsXXXm, variable == i)$value)$y)) %>% unlist %>% max

qplot(value, geom="density", data=lpsXXXm, fill=I("grey50"), 
      colour=I("grey45"), alpha=I(0.5)) + 
  facet_wrap(~variable) + xlim(c(0,1)) +
  theme_bw() +
  xlab("Posterior probability to pick plot") +
  ggtitle(sprintf("Pic ID %s (based on %.1f picks)", turk16.picks$pic_id[i],
                  sum(turk16.picks$counts[i][[1]]))) + 
  geom_text(data=dframe, aes(label = Bfactor), x = 1, y = 0.9*ymax, size = 5, 
            colour="grey70", hjust="inward", vjust="inward")

@
\end{subfigure}
\caption{\label{fig:xpl-124} Lineup (left) and posterior densities (right) corresponding to evaluations by independent observers.}
\end{figure}

\bibliographystyle{asa}
\bibliography{references}


\end{document}