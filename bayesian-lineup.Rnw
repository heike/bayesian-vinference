\documentclass[12pt]{article}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{multirow}
\usepackage{hyperref}
% \usepackage{endfloat} % Figures to the end of the document

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
%---------------------------------------------------
%                 Editing Commands
\newcommand{\hh}[1]{{\color{magenta} #1}}
\newcommand{\svp}[1]{{\color{darkgray} #1}}
\newcommand{\fix}[1]{{\color{blue} #1}}
\newcommand{\todo}[1]{{\color{purple} #1}}

%---------------------------------------------------
%                 Placing Figures
\usepackage[font=small,skip=5pt]{caption}
\usepackage{subcaption}
\usepackage{afterpage}
\usepackage{rotating}
\usepackage[dvipsnames,svgnames,table]{xcolor}
\graphicspath{{figure/}}

%---------------------------------------------------
% Define new environment
\newtheorem{theorem}{Theorem}[section]
\newtheorem{algorithm}[theorem]{Algorithm}
%---------------------------------------------------

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%


\begin{document}

% \bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}{#1}\small\normalsize}
\spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind {
  \title{\bf A Bayesian approach to visual inference}
  \author{Susan VanderPlas, Heike Hofmann\thanks{
    The authors gratefully acknowledge funding from the National Science Foundation Grant \# DMS 1007697. All data collection has been conducted with approval from the Institutional Review Board IRB 10-347}\hspace{.2cm}\\
    Department of Statistics and Statistical Laboratory, Iowa State University}%
  \maketitle%
} \fi

\if1\blind {
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf A Bayesian approach to visual inference}
\end{center}
  \medskip
} \fi

\section{Introduction}

% Introduction to graphical testing: Rorschach vs. Visual Inference

% Graphical Testing vs. Statistical Testing - power improvements w/ additional people

% Something about the power of the human visual system? Don't want to get too off topic...
<<setup, echo = F, include = F>>=
library(tidyverse)
knitr::opts_chunk$set(echo = F, message = F, warning = F)
# theme_set(theme_bw())
@

<<bf-code, include = F>>=
mvbeta <- function(alpha, log = F) {
  z <- sum(lgamma(alpha)) - lgamma(sum(alpha))
  if (!log) return(exp(z)) else return(z)
}

bf <- function(a1, a2, m = 20, c, k = sum(c)) {
  stopifnot(a1 > 0, a2 > 0, c <= k, m > 1)

  beta(a2, (m - 1)*a2) * beta(c + a1, k - c + (m - 1)*a1) /
    (beta(a1, (m - 1)*a1) * beta(c + a2, k - c + (m - 1)*a2))
}
bf_vec <- function(a1, a2, m = 20, c, k = sum(c)) {
  stopifnot(a1 > 0, a2 > 0, c <= k, m > 1)

 exp(mvbeta(rep(a2, length(c)), log = T) + mvbeta(a1 + c, log = T) -
   mvbeta(rep(a1, length(c)), log = T) - mvbeta(a2 + c, log = T))
}
vis_p_value <- function(C, K, alpha = 1, m = 20){
  single_p <- function(cc, kk, aa, mm) {
    x <- cc:kk
    sum(exp(lchoose(kk, x) - lbeta(aa, (mm - 1) * aa) + lbeta(x + aa, kk - x + (mm - 1) * aa)))
  }

  df <- tibble(cc = C,
               kk = K,
               aa = alpha,
               mm = m) %>%
    unnest() %>%
    mutate(p = purrr::pmap_dbl(., single_p))
  df$p
}
vis_p_value_orig <- function(C, K, m = 20){
  single_p <- function(cc, kk, aa, mm) {
    x <- cc:kk
    sum(exp(lchoose(kk, x) - x*log(mm) + (kk-x)*log(1-1/mm)))
  }

  df <- tibble(cc = C,
               kk = K,
               mm = m) %>%
    unnest() %>%
    mutate(p = purrr::pmap_dbl(., single_p))
  df$p
}
@

<<turk-studies-data,  include = F>>=
source("code/process_all_lineup_data.R")
@

<<lineup-bf-data-setup, cache = F, include = F, warning = F>>=
# t13plot_data <- read_csv("data/Turk13/lineups/data/data-1-1-1-20-2-14-5.csv")
#
# counts_turk13 <- studies_sum %>%
#   ungroup() %>%
#   filter(study == "turk13" & pic_id <=2) %>%
#   group_by(study, response_no) %>%
#   summarize(n = sum(n))%>%
#   mutate(bf = bf(1/2, 2, m = 20, c = n, k = sum(n))) %>%
#   mutate(.sample_inner = response_no)
#
# counts_t6_sig <- studies_sum %>%
#   ungroup() %>%
#   filter(study == "turk6" & pic_id == 1047) %>%
#   group_by(study, response_no) %>%
#   summarize(n = sum(n))%>%
#   mutate(bf = bf(1/2, 2, m = 20, c = n, k = sum(n))) %>%
#   mutate(.sample = response_no)
#
# t6plot_data <- read_csv("data/Turk6/turk6_n100_96_1_4.csv")
# library(beeswarm)
#
# t6_sig_data <- t6plot_data %>%
# 		  split(.$.sample) %>%
# 		  purrr::map2_dfr(., (1:length(.)) == 1, function(x, y) {
# 		    if (y) {
# 		      dev.new(width = 8, height = 8)
# 		      beeswarm(formula = vals~group, data = x, add = F, do.plot = T, method = "swarm", cex = 2.5)
# 		    }
# 		    bs <- beeswarm(formula = vals~group, data = x, add = F, do.plot = F, method = "swarm", cex = 2.5)
# 		    data.frame(vals = bs$y.orig, group = bs$x.orig, newy = bs$y,
# 		               newx = bs$x, .sample = x$.sample[1])
# 		  })
# dev.off()
#
# t6_ambig_data <- read_csv("data/Turk6/turk6_60_96_1_8.csv")
# counts_t6_ambig <- studies_sum %>%
#   ungroup() %>%
#   filter(study == "turk6" & pic_id == 995) %>%
#   group_by(study, response_no) %>%
#   summarize(n = sum(n))%>%
#   mutate(bf = bf(1/2, 2, m = 20, c = n, k = sum(n))) %>%
#   mutate(.sample = response_no)
#
# save(counts_t6_ambig, t6_ambig_data, t13plot_data, counts_turk13, t6_sig_data, counts_t6_sig, file = "data/turk6_13_examples.Rdata")
load("data/turk6_13_examples.Rdata")
@

Graphics provide the opportunity to understand statistical data at an intuitive level: we can gain more information about the relationship between two variables by considering a simple scatter plot than we might obtain from an entire day of generating numerical summaries. Graphics leverage the bandwidth of the visual system for implicit data analysis, and because this analysis is implicit, we often assume graphics are not decisive in the same way that an hypothesis test is decisive: generally, graphics do not come with a significance threshold, and in many cases, we do not explicitly construct the hypothesis we might be testing before viewing the chart.

Visual inference allows for us to test graphics as visual statistics: charts are, after all, a quantity computed from values in a sample of data. In order to test whether a chart shows a visually significant result, we can use the same philosophy used by randomization tests: construct a sampling method consistent with the null hypothesis, generate many copies of the test statistic (in this case, the plot), and see where the real statistic falls in the distribution of artificially generated quantites \citep{buja:2009}. An assembly of several null plots with a target (or data) plot is called a lineup, after the criminal procedure of the same name. Typically, lineups are composed of 19 ``null" plots (generated under the null hypothesis) and one data plot containing the real data.

Of course, with numerical statistics, there is a natural ordering to computed numerical quantities; with plots, we must run each statistic through another process (the visual system, or a facsimile thereof\footnote{Cite Giora's Deep Learning work, \url{http://giorasimchoni.com/deep_visual_inference.html}}) in order to evaluate significance. During this evaluation process, the user selects one or more plots from the lineup which are ``different" in some way (though some experiments may specify the particular feature under examination).

% TODO: explain different lineup scenarios here? Not sure if this is the best place for it, or if it's necessary at all.

Typically, graphical tests utilize a service like Amazon Mechanical Turk to acquire multiple evaluations of the same lineup; informally, if multiple individuals select the plot generated from the data rather than the null plots, the visual statistic is likely to be significant \citep{majumder2013validation}.

% TODO: add lineup here


While visual inference was initially developed to mimic frequentist hypothesis tests, using lineups of 20 plots with one data plot, so that the probability of selecting the data plot randomly is $p=0.05$, visual inference itself does not demand use of frequentist techniques. In this paper, we develop one possible framework for visual inference, using a Bayesian framework with a Dirichlet-multinomial distribution to model the probabilities of selecting each panel and a Multinomial distribution to model the observed participant selections. This framework for analysis of visual inference data has been in use as part of the \texttt{vinference} package~\citep{vinference} for several years \citep{loyAreYouNormal2015,loy2016variations,vanderplas:2017}, but has not been formally described in any publication. Here, we provide the mathematical foundation for the multinomial-dirichlet model used in the analysis of visual inference experiments, exploring the implications of the model and proposing a modification which better describes the perceptual process of lineup evaluation.

\section{Lineup Model Specification}

We will begin with a generic $m$-panel lineup, with selection probabilities $\theta_i, i = 1, ..., m$ where $\sum_{i=1}^m \theta_i = 1$, that is, the participant will select one (and only one) panel from the lineup as the most different. Our lineup has been evaluated by $K$ individuals, with $c_i, i = 1, ..., m$ the selection count for each panel, and $K = \sum_{i=1}^m c_i$.

A natural data model for this data is the Multinomial distribution, which has parameters $N, \bm{\theta}$, where $N$ describes the total number of events (that is, $N=K$) and $\bm{\theta} = \theta_1, ..., \theta_m$ describes the probabilities of each event occurring. We will fix $K$, as that is controlled by the experimental design, and model $\bm{\theta}$.

\begin{align}\label{eqn:multinomial-pmf}
f(\bm{c}|K, \bm{\theta}) & = \frac{K!}{c_1! \cdots c_m!} \prod_{i=1}^m \theta_i^{c_i}
\end{align}

We assign prior probability to $p$ using a Dirichlet distribution with concentration hyperparameter $\bm{\alpha}$, which happens to be conjugate to the multinomial distribution. As the position of the panels within the lineup are random, we use a symmetric Dirichlet distribution, with $\alpha_i = \alpha, i = 1, ..., m$, that is, the concentration hyperparameter is constant. This allows us to vary the lineup difficulty through the hyperparameter $\alpha$ without having to specify which plot $i$ is the target plot.

The pdf of the symmetric Dirichlet distribution is
\begin{align}\label{eqn:dirichlet-pdf}
f(\bm{\theta}|\alpha) & = \frac{\left(\Gamma(\alpha)\right)^m}{\Gamma(m\alpha)} \prod_{i=1}^m \theta_i^{\alpha - 1}
\end{align}

Using the conjugate relationship between the Dirichlet and Multinomial distributions, we then get the posterior distribution as the Dirichlet$(\bm{c + \alpha})$ distribution.

\begin{align}\begin{split}\label{eqn:full-model-specification}
(\alpha_1, ..., \alpha_m) = \bm{\alpha} &= \text{concentration hyperparameter}\\
(c_1, ..., c_m) = \bm{c} &= \text{observed plot selections}, \sum_{i=1}^m c_i = K\\
p(\bm{\theta}) &\sim Dirichlet(\bm\alpha) \\
f(\bm{c} | \bm{\theta}) & \sim Multinomial(\bm\theta, K)\\
p(\bm\theta | \bm{c}, \bm{\alpha}) & \sim Dirichlet(\bm{c} + \bm{\alpha})
\end{split}\end{align}

where $Multinomial(\bm\theta, K)$ is defined as in \autoref{eqn:multinomial-pmf} and $Dirichlet(\bm\alpha)$ is defined as in \autoref{eqn:dirichlet-pdf}.

Typically, when evaluating lineups, we compare the number of target plot identifications with the aggregate number of null plot identifications (see \citet{majumder2013validation}). This is equivalent to the marginal distribution of $c_t$, where $t \in 1, ..., m$ is the index of the target panel in the lineup; that is, in \citet{majumder2013validation}, a binomial distribution, and in this formulation, a beta-binomial distribution.

\begin{align}\begin{split}\label{eqn:marginal-model-specification}
\alpha &= \text{concentration hyperparameter}\\
c_t &= \text{target plot selections},\\
K &= \text{total evaluations}\\
p(\theta_t) &\sim Beta(\alpha, (m-1)\alpha) \\
f(c_t | \theta_t) & \sim Binomial(\theta_t, K)\\
p(\theta_t | \bm{c_t}, \alpha) & \sim Beta(c_t + \alpha, K - c_t + (m-1)\alpha)
\end{split}\end{align}

While we have used the vocabulary of a Bayesian model, it should be noted that this modeling strategy is actually philosophically agnostic: it would be equally reasonable to think of this as an overdispersed multinomial model.

Examining the parameters of either the full or marginal model specifications in \autoref{eqn:full-model-specification} and \autoref{eqn:marginal-model-specification}, it is evident that $\alpha$ provides the equivalent of pseudo-observations for each plot; that is, the effect of $\alpha$ is equivalent to adding $\alpha$ identifications to each panel in the lineup. When $\alpha$ is small, these pseudo-observations have relatively little influence, but when $\alpha$ is large, the pseudo-observations can quickly dwarf any information provided by the data. This is particularly true for the marginal Beta-Binomial model, where the equivalent of $(m-1)\alpha$ pseudo-observations are added. In most lineup studies, a plot might be evaluated between 10 and 30 times; with a $m=20$ lineup, even $\alpha = 1$ can easily dominate the participant selection data.

In addition to the pseudo-observation interpretation, $\alpha$ provides information about the number of panels in a lineup which are likely to attract participant interest. It is useful to detour slightly from the discussion of visual inference to explore the impact and interpretation(s) of $\alpha$ in the context of statistical lineups.

\section{Dirichlet Hyperparameter}\label{sec:alpha}
% Stolen from heike/lineup-scenarios
When $\alpha = 1$, the symmetric Dirichlet distribution is uniform on the $m-1$ dimensional simplex. When $\alpha < 1$, the mass of the distribution is along the edges of the simplex, where most values of $\theta_i$ will be close to 0. When $\alpha>1$, the mass of the distribution is in the center of the simplex, with most of the $\theta_i$ having similar values. \autoref{fig:simplex} shows ternary plots~\citep{ggtern} of values simulated from a 3-dimensional Dirichlet distribution which illustrate the effect of $\alpha$ on the sampled $\bm\theta$.

<<simplex, echo=FALSE, out.width='\\textwidth', fig.width=12, fig.height=4, fig.cap = "Dirichlet distributed samples on the 2-dimensional simplex.">>=
simplex <- purrr::map_df(.x = c(1/3, 1, 3), ~data.frame(
  gtools::rdirichlet(5000, alpha = rep(.x, 3))
), .id = "alpha") %>%
  rename(p1 = X1, p2 = X2, p3 = X3) %>%
  mutate(alphalabel = factor(alpha, levels = 1:3, labels = c("alpha: 1/3", "alpha: 1", "alpha: 3")))

ggtern::ggtern(aes(x = p1, y = p2, z = p3), data = simplex) +
  geom_point(alpha = .075, shape = 16) +
  facet_wrap(~alphalabel, labeller = "label_parsed") +
  theme(panel.background = element_rect(colour = "black"))
@
% End stolen from heike/lineup-scenarios

<<set-theme-bw, include = F>>=
theme_set(theme_bw())
@

While graphical illustrations of the 20-dimensional dirichlet distribution are more difficult, we can use prior predictive simulations to assess the meaning of $\alpha$ as it relates to how many panels in a lineup attract participant attention. \autoref{fig:prior-predictive} shows simulated $c_i$ counts (sorted for visual clarity) for several values of $\alpha$; it is evident that for $\alpha<0.05$ only one panel of the lineup receives significant attention, while for $\alpha> .25$, participant attention is divided among several interesting panels of the lineup.

<<prior-predictive, echo = F, fig.cap = "Prior predictive distribution of number of participant selections of panels $c_i$ (sorted by frequency) for different values of $\\alpha$, with fixed $K=20$ plot evaluations. Low values of $\\alpha$ have fewer plots with any participant selections, while higher values of $\\alpha$ have more plots with participant selections.", fig.width = 8, fig.height = 4, out.width = "\\textwidth" >>=

sim_lineup_model <- function(alpha, m = 20, k = 20, N = 50) {
  theta <- gtools::rdirichlet(1, rep(alpha, m))
  sels <- rmultinom(N, size = k, prob = theta)
  sels
}

alphas <- c(.001, .02, .05, .1, .5, 1, 2, 5, 20, 1000)
prior_pred <- tibble(alpha = alphas,
                     plot_sels = purrr::map(alpha, sim_lineup_model, N = 100)) %>%
  mutate(
    sel_ordered = purrr::map(plot_sels, ~apply(., 2, sort, decreasing = T)),
    sel_ordered_long = purrr::map(
      sel_ordered,
      ~tibble(idx = rep(1:nrow(.x), times = ncol(.x)),
              rep = rep(1:ncol(.x), each = nrow(.x)),
              sels = as.vector(.x, mode = "numeric")))
  ) %>%
  select(-plot_sels, -sel_ordered) %>%
  unnest() %>%
  arrange(alpha) %>%
  mutate(label = sprintf("alpha == %f", alpha) %>% factor(levels = sprintf("alpha == %f", alphas), ordered = T))

ggplot(prior_pred) +
  geom_path(aes(x = idx, y = jitter(sels, amount = .4), group = interaction(rep, alpha)), alpha = .05) +
  facet_wrap(~label, labeller = label_parsed, nrow = 2) +
  scale_x_continuous("Ordered panel number") +
  scale_y_continuous("# Simulated Panel Selections (of 20 evaluations)")

@

% A modification of the frequentist paradigm developed in \citet{majumder2013validation} and released as a technical report \todo{get citation!} simulates p-values from a Dirichlet-Multinomial distribution with $\alpha=1$; based on the prior predictive distributions alone, $\alpha=1$ does not appear to fit the observed data or theoretical process (e.g. the tendency to identify even small differences from among similar things) well.

The data model used in \citet{majumder2013validation} assumes $\theta_t = 1/m$, that is, $\theta_t$ is fixed and equal to the selection probability of every other panel in the lineup. This assumption, which would correspond to infinite $\alpha$, does not match our experience when evaluating a lineup, nor the accumulated experimental evidence which shows that even null plots do not show equal selection probabilities for each panel. When examining a lineup, we are generally drawn immediately to 1-4 panels, and the remaining evaluation is to decide between those panels; we also know that typically the same panels are selected across multiple individuals. To account for this issue, recent analyses of lineups calculate visual p-values using the mass function
\begin{align}
P(C\geq x) = \sum_{x = C}^{K} \binom{K}{x} \frac{1}{B(\alpha, (m-1)\alpha)}\cdot B(x+\alpha, K-x+(m-1)\alpha)\label{eqn:sim-pmf}
\end{align}
where $C$ is the number of data panel detections and $K$ is the number of independent evaluations of the lineup~\citep{vinference,vanderplas:2017,loyAreYouNormal2015}. The \texttt{vinference} package calculated visual p-values by simulating draws of $\theta$ from a uniform distribution (which corresponds to an assumption that $\alpha=1$) but the more general solution is useful to consider, as we may not actually believe $\theta$ is uniformly distributed over the $(m - 1)$ simplex.

In the past, we have compared this visual p-value to a threshold of 0.05, and if it is smaller, we rejected the hypothesis that every plot is equally likely to be selected. It is not necessary to rely on hypothesis testing for this analysis; an equally valid approach would be to compare two competing models: one with $\alpha \ll 1$, which allows for a smaller set of panels to be selected, and one with $\alpha \gg 1$, which would correspond to a more equal theoretical distribution of panel selections, using a Bayes Factor to quantify the evidence in favor of either model. This approach is derived in the next section.
% Now that we have some intuition for the effect of the value of $\alpha$, we can proceed with deriving a Bayes Factor for visual inference analysis.

\section{Bayes Factors for Lineups}\label{sec:bayesfactors}
Bayes factors are somewhat analogous to frequentist hypothesis tests~\citep{kassBayesFactors1995}, but to construct a Bayes factor for a lineup, we need two models to compare.
In order to construct a Bayes Factor which would be roughly equivalent to the standard lineup evaluation, we would compare two models with different hyperparameters: the model structure is the same, but in the first model, we would expect one or two of the panels to be selected more frequently (corresponding to a low value of $\alpha$); in the second, we would expect all panels to be selected approximately equally (corresponding to a high value of $\alpha$).

\begin{description}
\item[M1] $\alpha < 1$ - a small value of $\alpha$, indicating that one or two of the plots might tend to be selected more frequently.
\item [M2] $\alpha \geq 1$ - a large value of $\alpha$, suggesting that each plot is (approximately) equally likely to be selected.
\end{description}

\autoref{fig:prior-predictive} shows the sorted panel selection counts for several values of $\alpha$; note that even at $\alpha=1000$, all plots are not selected equally (the equivalent of the null hypothesis in the visual p-value calculation in \citet{majumder:2013}).

As with the direct p-value calculation, we will first consider the construction of a Bayes Factor for an entire lineup (e.g. all 20 panels), and then we will consider the marginal Bayes factor which is more comparable to the approach used to assess lineups in previous studies~\citep{majumder2013validation} (see  \autoref{eqn:sim-pmf}).

\subsection{Full Lineup Bayes Factors}

A full lineup Bayes factor would test the selection probabilities of the entire set of $m$ panels as a single unit, with no differentiation based on whether the panel shows null or target data. The multidimensional Bayes Factor for a lineup tests whether one or more panels are more likely to be selected (compared to a model where all panels are equally likely to be selected); it does not indicate which panel(s) are more likely. As such, it is of somewhat limited utility in the analyses of lineups which have been conducted to date, but is provided here to support the later calculation of a marginal Bayes factor which is analogous to the visual p-value calculation.

Let $\alpha_{M_1} < 1$ be the prior concentration hyperparameter for model 1, and $\alpha_{M_2} \geq 1$ be the prior concentration hyperparamer for model 2. If we assume that both models are equally likely a priori, then the prior model odds cancel, and we can derive the bayes factor comparing model 1 to model 2 for an entire lineup as:

% Derive bayes factors for entire lineups
\begin{align}
BF(M_1, M_2) = &  \frac{\displaystyle\int_\theta p(\theta | \alpha_{M_1}) f(\bm{c} | \bm\theta, \alpha_{M_1}) d\bm\theta}
                         {\displaystyle\int_\theta p(\theta | \alpha_{M_2}) f(\bm{c} | \bm\theta, \alpha_{M_2}) d\bm\theta} \nonumber\\
              =  &  \frac
              {\int\left(\frac{1}{B(\bm\alpha_1)} \prod_{i=1}^m \theta_i^{\alpha_1 - 1}\right)\left(\frac{\left(\sum_{i=1}^m c_i\right)!}{c_1!\times \cdots\times c_m!} \prod_{i=1}^m \theta_i^{x_i}\right) d\bm\theta}
              {\int\left(\frac{1}{B(\bm\alpha_2)} \prod_{i=1}^m \theta_i^{\alpha_2 - 1}\right)\left(\frac{\left(\sum_{i=1}^m c_i\right)!}{c_1!\times \cdots\times c_m!} \prod_{i=1}^m \theta_i^{x_i}\right) d\bm\theta}\\
              =  & \frac{B(\bm\alpha_2)}{B(\bm\alpha_1)}
              \frac{\frac{\left(\sum_{i=1}^m c_i\right)!}{c_1!\times \cdots\times c_m!}}{\frac{\left(\sum_{i=1}^m c_i\right)!}{c_1!\times \cdots\times c_m!}}
              \frac{\int \prod_{i=1}^m \theta_i^{\alpha_1 - 1 + c_i}d\bm\theta}{\int \prod_{i=1}^m \theta_i^{\alpha_2 - 1 + c_i}d\bm\theta}\\
              = & \frac{B(\bm\alpha_2)B(\bm\alpha_1+\bm{c})}{B(\bm\alpha_1)B(\bm\alpha_2+\bm{c})}
\end{align}

While the multidimensional Bayes factor is not analogous to the frequentist visual p-value, it does provide the foundation for the calculation of a marginal Bayes factor that is similar to the p-value calculation in \autoref{eqn:sim-pmf}.

\subsection{Marginal Bayes Factors for Target Plots}

Working from the Beta-Binomial marginal distribution in \autoref{eqn:marginal-model-specification}, we can similarly derive a marginal Bayes factor that could be calculated for any panel $i$ in the $m$-panel lineup. In most cases, it would not be particularly useful to calculate a Bayes factor for null plots in the lineup; as in the visual p-value calculation, we are primarily concerned with the number of data detections compared to the aggregate number of null plot detections. Supposing that the data is shown in panel $i$, we can derive the Bayes factor for a  single target lineup as:

% Derive bayes factors for plots based on one-target lineups
\begin{align}
BF(M_1, M_2)_i  =& \frac{\displaystyle\int_\theta p(\theta | \alpha_{M_1}) f(K, c_i | \theta, \alpha_{M_1}) d\theta}
                        {\displaystyle\int_\theta p(\theta | \alpha_{M_2}) f(K, c_i | \theta, \alpha_{M_2}) d\theta} \nonumber\\
= &\frac{\displaystyle\int_\theta \binom{K}{c_i} \theta^{c_i} (1 - \theta)^{K - c_i}
         \frac{1}{B(\alpha_{M_1}, (m-1) \alpha_{M_1})} \theta^{\alpha_{M_1}} (1-\theta)^{(m-1) \alpha_{M_1}} d\theta}
        {\displaystyle\int_\theta \binom{K}{c_i} \theta^{c_i} (1 - \theta)^{K - c_i}
         \frac{1}{B(\alpha_{M_2}, (m-1) \alpha_{M_2})} \theta^{\alpha_{M_2}} (1-\theta)^{(m-1) \alpha_{M_2}} d\theta}\nonumber\\
= &\frac{B(\alpha_{M_2}, (m-1)\alpha_{M_2})}{B(\alpha_{M_1}, (m-1)\alpha_{M_1})}
   \frac{B(c_i + \alpha_{M_1}, K - c_i + (m-1)\alpha_{M_1})}{B(c_i + \alpha_{M_2}, K - c_i + (m-1)\alpha_{M_2})} \times \nonumber\\
  &\frac{\displaystyle\int_\theta \frac{1}{B(c_i + \alpha_{M_1}, K - c_i + (m-1)\alpha_{M_1})} \theta^{c_i + \alpha_{M_1} - 1} (1-\theta)^{K - c_i + (m-1)\alpha_{M_1} - 1} d\theta}
        {\displaystyle\int_\theta \frac{1}{B(c_i + \alpha_{M_2}, K - c_i + (m-1)\alpha_{M_2})} \theta^{c_i + \alpha_{M_2} - 1} (1-\theta)^{K - c_i + (m-1)\alpha_{M_2} - 1} d\theta}\nonumber\\
 = & \frac{B(\alpha_{M_2}, (m-1)\alpha_{M_2})}{B(\alpha_{M_1}, (m-1)\alpha_{M_1})}
\frac{B(c_i + \alpha_{M_1}, K - c_i + (m-1)\alpha_{M_1})}{B(c_i + \alpha_{M_2}, K - c_i + (m-1)\alpha_{M_2})}
\end{align}

A similar derivation replacing $m-1$ with $m_0$, the number of null plots in the lineup, could be used to analyze modifications of the single-target lineup design, such as the study in \citet{vanderplas:2017}.

The marginal beta-binomial model considers the data detections and the aggregated null-plot detections; as a result, the parameters of the posterior beta distribution are $C + \alpha, K - C + (m-1)\alpha$. As discussed in \autoref{sec:alpha}, $\alpha$ can be thought of as providing pseudo-observations for each plot; that is, the effect of $\alpha$ is equivalent to adding $\alpha$ observations to each panel in the lineup.

\subsection{Selection of $\alpha_{M_1}$ and $\alpha_{M_2}$}

The marginal and full Bayes factors derived above depend on parameters $\alpha_{M_1}$ and $\alpha_{M_2}$, where in our formulation $\alpha_{M_1} < \alpha_{M_2}$. We could choose values of $\alpha_{M_{1,2}}$ based on the prior predictive distributions shown in \autoref{fig:prior-predictive}, but it is also useful to assess the effect of different combinations of $\alpha_{M_{1,2}}$ on the resulting Bayes Factor.

\autoref{fig:bayes-factors-alphas} shows Bayes Factors in the marginal case for combinations of $\alpha_1, \alpha_2$ over several different $C$ values, assuming a total of $K=20$ independent evaluations.
<<bayes-factors-alphas, fig.width = 8, fig.height = 4, fig.cap = "Bayes factor values at different levels of $\\alpha_1$ and $\\alpha_2$. If there are large numbers of data panel identifications, the Bayes Factor will be large even with conservative parameter values (e.g. $\\alpha_1$ near 1, $\\alpha_2 =  2$). The choice of alphas is much more impactful when there are fewer data plot identifications. While the criteria for interpretation of Bayes factors vary, a result of $>20$ is generally considered to correspond to strong evidence toward $M_1$ over $M_2$." >>=

breaks <-  c(2, 5, 10, 20)
data_breaks <- c(1, 3, 5, 8, 10)


p01 <- tidyr::crossing(a1 = seq(.01, 1, .01),
                a2 = breaks,
                m = 20,
                k = 20,
                c = data_breaks) %>%
  mutate(bf = purrr::pmap_dbl(., bf)) %>%
  mutate(a2f = factor(a2, levels = unique(a2), labels = sprintf("alpha[2] == %f", unique(a2)), ordered = T)) %>%
  ggplot(aes(x = a1, y = bf, color = factor(c), group = factor(c))) +
  geom_line() +
  scale_y_log10("Bayes Factor", breaks = c(1, 10, 100, 1000, 10000)) +
  scale_x_continuous(expression(alpha[1])) +
  scale_color_discrete("# Data\nPanel\nIdentifications\n(K = 20)", guide = F) +
  facet_wrap(~a2f, labeller = "label_parsed")

p02 <- tidyr::crossing(a2 = seq(1.5, 20, by = 0.1),
                a1 = 1/breaks,
                m = 20,
                k = 20,
                c = data_breaks) %>%
  mutate(bf = purrr::pmap_dbl(., bf)) %>%
  mutate(a2f = factor(a2, levels = unique(a2),
                      labels = sprintf("alpha[2] == ~~%f", unique(a2)),
                      ordered = T)) %>%
  mutate(a1f = factor(a1, levels = 1/rev(breaks),
                     labels = sprintf("alpha[1] == ~~1/%f", rev(breaks)),
                     ordered = T)) %>%
  ggplot(aes(x = a2, y = bf, color = factor(c), group = factor(c))) +
  geom_line() +
  scale_y_log10("Bayes Factor", breaks = c(1, 10, 100, 1000, 10000)) +
  scale_x_continuous(expression(alpha[2])) +
  scale_color_discrete("# Data\nPanel\nIdentifications\n(K = 20)") +
  facet_wrap(~a1f, labeller = "label_parsed")

gridExtra::grid.arrange(p01, p02, nrow = 1, widths = c(.425, .575))
@
Clearly, the choice of $\alpha_1$ and $\alpha_2$ is important, but in order to get a feel for how the Bayes factor approach might compare to the visual p-value approach, let us set $\alpha_1 = 0.5$ and $\alpha_2 = 2$, which would correspond to relatively weak hyperparameters for both models, and examine the results from three lineups.
The first of these examples, shown in \autoref{fig:turk13-lineup}, is from \citet{loyAreYouNormal2015}, a study which compared different q-q plots.
We can see that there is no significant support for the choice of model 1 over model 2, or vice versa, when considering the target panel selections compared to the aggregate null panel selections.
The target panel, which is in panel 14, had relatively few selections, resulting in a marginal Bayes Factor of 1.03.
The full-model Bayes Factor of \Sexpr{bf_vec(1/2, 2, c = counts_turk13$n)} indicates that there is some evidence that all plots are not equally likely to be selected, primarily due to the multiple selections of a null plot in panel 4.
The marginal Bayes Factor for panel 4 is \Sexpr{bf(1/2, 2, c = counts_turk13$n)[4]}, which would correspond to weak support for $M_1$ over $M_2$ if panel 4 had been contained the data.
The corresponding visual p-value for this plot calculated from the binomial distribution~\citep{majumder2013validation} is \Sexpr{sprintf("%0.4g", vis_p_value_orig(counts_turk13$n, sum(counts_turk13$n))[14])} and the beta-binomial p-value with $\alpha = 1$ is \Sexpr{sprintf("%0.4g", vis_p_value(counts_turk13$n, sum(counts_turk13$n))[14])}.
In this example, the conclusion would be the same for frequentist and Bayesian tests of the hypothesis that every plot is approximately likely to be selected.

<<turk13-lineup, fig.cap = sprintf("A lineup from \\citet{loyAreYouNormal2015} which was evaluated a total of %d times. Number of selections $c_i$ are shown at the top left of each panel; the calculated Bayes Factors with $\\alpha_1 = 0.5$ and $\\alpha_2 = 2$ is shown at the bottom right of the target panel (panel 14). As panel 4 was selected more frequently than panel 14, there is no significant information in favor of model 1 (the target plot is more likely to be selected than the nulls) over model 2. The multidimensional (whole plot) bayes factor with the same $\\alpha_1,\\alpha_2$ values is %.2f, indicating that there is weak to moderate evidence that all plots are not equally likely to be selected; that is, that model 1 better aligns with the observed data than model 2. Note that this evidence support is primarily due to the multiple selections of panel 4, which shows data generated by the null data model.", sum(counts_turk13$n), bf_vec(1/2, 2, c = counts_turk13$n)), fig.width = 6, fig.height = 6/5*4.5, fig.align='center', out.width = ".66\\textwidth">>=
t13plot_data %>%
  ggplot() +
  geom_ribbon(aes(x = naive1.env.fit.value, ymin = naive1.env.lower, ymax = naive1.env.upper), alpha = .1) +
  geom_point(aes(x = naive1.qq.x, y = naive1.qq.y)) +
  geom_abline(aes(slope = 1, intercept = 0), color = "grey30") +
  geom_text(aes(x = -Inf, y = Inf, label = sprintf("c[%d] == %.0f", .sample_inner, n)),
            hjust = -0.2, vjust = 1, parse = T, data = counts_turk13, inherit.aes = F) +
  geom_text(aes(x = Inf, y = -Inf, label = sprintf("BF == %.2f", bf)),
            vjust = -0.2, hjust = 1, parse = T, data = filter(counts_turk13, .sample_inner == 14), inherit.aes = F) +
  facet_wrap(~.sample_inner) +
  theme(axis.text = element_blank(), axis.title = element_blank(), axis.ticks = element_blank())
@

One advantage of the Multinomial-Dirichlet model presented in this paper relative to the strict binomial method is that it seamlessly incorporates multiple target plot selections: a participant who selects $p$ panels of a lineup would contribute $1/p$ votes to each panel's total.

Participants selected the data plot in \autoref{fig:turk13-lineup} in \Sexpr{counts_turk13$n[14]} of \Sexpr{sum(counts_turk13$n)} evaluations; we should also examine how the Bayes factor performs when the null plots are much less likely to be selected. \autoref{fig:turk6-lineup} shows a lineup with overwhelming evidence suggesting that the target plot is disproportionately selected.
<<turk6-lineup, fig.cap = sprintf("A lineup from \\todo{cite?} which was evaluated a total of %d times. Number of selections $c_i$ are shown at the top left of each panel; the calculated Bayes Factors with $\\alpha_1 = 0.5$ and $\\alpha_2 = 2$ is shown at the bottom right of the target panel (panel 4). The multidimensional (whole plot) bayes factor with the same $\\alpha_1,\\alpha_2$ values is %9.3e, indicating that there is overwhelming evidence that all plots are not equally likely to be selected; that is, that model 1 better aligns with the observed data than model 2.", sum(counts_t6_sig$n), bf_vec(1/2, 2, c = counts_t6_sig$n)), fig.width = 6, fig.height = 6/5*4.5, fig.align='center', out.width = ".66\\textwidth">>=
ggplot(t6_sig_data, aes(x = group, y = vals)) +
  geom_boxplot(outlier.colour = 'NA', colour = "grey60", width = 0.75) +
  geom_point(aes(x = newx, y = newy, colour = factor(group)), shape = 1, size = 1) +
  scale_color_discrete(guide = F) +
  facet_wrap(~.sample, ncol = 5) +
  theme(axis.title = element_blank(), axis.text = element_blank(), axis.ticks = element_blank()) +
  xlab("") + scale_y_continuous(expand = c(0.1,0)) +
  geom_text(aes(x = -Inf, y = Inf, label = sprintf("c[%d] == %.0f", .sample, n)),
            hjust = -0.2, vjust = 1, parse = T, data = counts_t6_sig, inherit.aes = F) +
  geom_text(aes(x = Inf, y = -Inf, label = sprintf("BF == %9.3e", bf)),
            vjust = -0.2, hjust = 1, parse = T, data = filter(counts_t6_sig, .sample == 4), inherit.aes = F)
@
The binomial model visual p-value for this plot calculated as in \citet{majumder2013validation} is \Sexpr{sprintf("%0.4g", vis_p_value_orig(counts_t6_sig$n, sum(counts_t6_sig$n))[4])} and the beta-binomial p-value with $\alpha = 1$ is \Sexpr{sprintf("%0.4g", vis_p_value(counts_t6_sig$n, sum(counts_t6_sig$n))[4])}. The amount of evidence suggesting that the hypothesis that all plots are equally likely to be selected is staggering no matter which method is employed.


<<turk6-ambiguous, fig.cap = sprintf("A lineup from \\todo{cite?} which was evaluated a total of %d times. Number of selections $c_i$ are shown at the top left of each panel; the calculated Bayes Factors with $\\alpha_1 = 0.5$ and $\\alpha_2 = 2$ is shown at the bottom right of the target panel (panel 8). The multidimensional (whole plot) bayes factor with the same $\\alpha_1,\\alpha_2$ values is %9.3e, indicating that there is overwhelming evidence that all plots are not equally likely to be selected; that is, that model 1 better aligns with the observed data than model 2.", sum(counts_t6_sig$n), bf_vec(1/2, 2, c = counts_t6_sig$n)), fig.width = 6, fig.height = 6/5*4.5, fig.align='center', out.width = ".66\\textwidth">>=
ggplot(t6_ambig_data, aes(y = vals, fill = factor(group))) +
  geom_boxplot() +
  scale_fill_discrete(guide = F) +
  facet_wrap(~.sample, ncol = 5) +
  theme(axis.title = element_blank(), axis.text = element_blank(), axis.ticks = element_blank()) +
  xlab("") + scale_y_continuous(expand = c(0.1,0)) +
  geom_text(aes(x = -Inf, y = Inf, label = sprintf("c[%d] == %.0f", .sample, n)),
            hjust = -0.2, vjust = 1, parse = T, data = counts_t6_ambig, inherit.aes = F) +
  geom_text(aes(x = Inf, y = -Inf, label = sprintf("BF == %9.3e", bf)),
            vjust = -0.2, hjust = 1, parse = T, data = filter(counts_t6_ambig, .sample == 8), inherit.aes = F)
@

Finally, it is good to assess the Bayes factor's performance on lineups which are somewhat ambiguous, with selections of the target plot as well as frequent selection of a null plot. Such a lineup is shown in \autoref{fig:turk6-ambiguous}, where panel 8 is the target panel and has a Bayes Factor of \Sexpr{sprintf("%.2f", filter(counts_t6_ambig, .sample == 8)$bf)}. The corresponding binomial visual p-value is \Sexpr{sprintf("%.4g", vis_p_value_orig(counts_t6_ambig$n, sum(counts_t6_ambig$n))[8])} and the beta-binomial visual p-value with $\alpha=1$ is \Sexpr{sprintf("%.4g", vis_p_value(counts_t6_ambig$n, sum(counts_t6_ambig$n))[8])}. All methods provide some indication that the panels are not equally likely to be selected, but the support from the Bayes Factor might be best stated as ``moderate" while the p-values would be considered ``highly significant" by most standards. This is much more consistent with a ``gut check" when considering the selection counts in \autoref{fig:turk6-ambiguous}; it is certainly not reasonable to believe that all panels are equally likely to be selected, as 14 of the 20 panels were not selected at all, but the target plot was selected less than half of the time, which does not seem to correspond to ``overwhelming evidence" that the target plot is more likely to be selected.

The Bayes factors presented in this section provide another way to evaluate visual inference experiments, but \svp{make the point that alpha selection can make a fairly big difference... }
\section{Hyperparameter Selection}
% As a result, it's important to understand the effect of $\alpha$ on the posterior distribution.

We know from experience as well as cognitive principles that it is unreasonable to assume that the selection probability of every null plot is precisely equal: null plots are randomly generated, and occasionally, the randomly generated plot will have an interesting feature (that may or may not be present in the target plot).
When that occurs, the interesting null plot will be selected more frequently than the other nulls, despite being generated by the same distribution.
The ability to identify stimuli as being different from one another is a fundamental part of cognition; the abstractions that allow us to use the terms `same' and `different' are fundamental to human intelligence~\citep{mingWhenThingsAre2017}.
As a result, when presented with a lineup, we will typically gravitate towards one or two panels which are different from our mental representation of a generalized panel on some measure, though not always the measure that's under investigation.
In the theoretical Multinomial-Dirichlet model proposed in this paper, the number of panels which could be expected to be visually different in a generalized lineup is a function of $\alpha$, the hyperparameter in \autoref{eqn:dirichlet-pdf} and \autoref{eqn:full-model-specification}.
In practice, we would create a null plot generating model first, and set $\alpha$ according to the perceived difficulty of lineup evaluation using the null generating model in question.
A difficult lineup, with many potentially interesting panels, would have a higher $\alpha$ value than an easy lineup with no null panels which were visually salient relative to the data panel.



Under the frequentist paradigm, $\alpha$ plays a similar role in modulating the visual p-value calculated from \autoref{eqn:sim-pmf}: lower $\alpha$ values produce a higher visual p-value estimate, and higher $\alpha$ values produce a lower visual p-value estimate.

<<vis-p-val-sensitivity-initial, fig.cap = "Sensitivity of visual p-value to selection of $\\alpha$ under the beta-binomial model. Corresponding values for the binomial model are shown on the right side of the plot; as $\\alpha \\rightarrow\\infty$, the beta-binomial p-value converges to the binomial model p-value.", fig.width = 8, fig.height = 5, out.width = "\\textwidth">>=
alphas <- exp(seq(-6, 6, by = .01))
data_breaks <- c(1:5, 6, 8, 10, 15, 20)


pv <- tidyr::crossing(alpha = alphas, C = data_breaks, K = 20) %>%
  mutate(p = vis_p_value(C, K, alpha))
pv2 <- tidyr::crossing(C = data_breaks, K = 20) %>%
  mutate(p = vis_p_value_orig(C, K))

ggplot(pv, aes(x = alpha, y = p, color = factor(C), group = factor(C))) +
  geom_line(size = 1) +
  geom_point(aes(x = exp(6.25), y = p, color = factor(C), shape = "Binomial\np-value"), data = pv2) +
  scale_y_continuous("Visual p-value") +
  scale_x_continuous(expression(alpha), trans = "log10", breaks = c(0.001, 0.01, .1, 1, 10, 100),
                     labels = c("0.001", "0.01", "0.1", "1", "10", "100")) +
  scale_color_brewer("# Data\nPanel\nIdentifications\n(K = 20)", palette = "Paired") +
  scale_shape_discrete("") +
  geom_hline(yintercept = 0.05, color = "grey") +
  guides(color = guide_legend(override.aes = list(shape = NA))) +
  annotate("segment", x = exp(6.25), xend = exp(6.25), y = -Inf, yend = .7, color = "grey", alpha = .5) +
  annotate("text", x = exp(6.25), y = 0.75,
           label = "Binomial\nmodel", vjust = 1, hjust = .75) +
  annotate("segment", x = 1, xend = 1, y = -Inf, yend = .7, color = "grey", alpha = .5) +
  annotate("text", x = 1, y = 0.75,
           label = "Beta-Binomial\nmodel", vjust = 1, hjust = .75) +
  theme_bw()
@

Clearly, the choice of $\alpha$ is critical under both the frequentist and Bayesian model paradigms. From a practical perspective, the number of plots which are visually salient and thus likely to be selected by participants is a factor of the lineup design (zero, one, or two targets), null plot generation method, and possibly the form of the plot (aesthetics, geometric representations, scales). While the prior predictive distributions in \autoref{fig:prior-predictive} are illustrative, in practice, we do not have a particularly good instinct for what a reasonable value of $\alpha$ would be for a particular lineup generation method. In the next section, we derive a method for estimating $\alpha$ from Rorshach or standard lineups and discuss possible uses for this method in assessing null plot generation and lineup difficulty.

\section[Estimating alpha]{Estimating $\alpha$}
\subsection{Derivation of MLE for $\alpha$}
% modified from heike/lineup-scenarios
In order to estimate $\alpha$ from the null plots of a lineup, let $m_0$ be the number of null plots in an $m$- panel lineup. If the lineup is a Rorshach lineup, then $m=m_0$. We can estimate $\alpha$ using a set of such lineups, $j=1, ..., n$. The likelihood function is then
\begin{align}
\mathscr{L}(\alpha|\theta) &= \prod_{j=1}^n \left(\frac{1}{B(\alpha)}\right)^{m_0} \prod_{i=1}^{m_0} \theta_{ij}^{\alpha - 1}\nonumber\\
& = \left(\frac{\Gamma(\alpha m_0)}{\left(\Gamma(\alpha)\right)^{m_0}}\right)^n \prod_{ij} \theta^{\alpha-1}_{ij}
\end{align}
and the derivative of the log-likelihood function can be calculated as
\begin{align}
\ln \mathscr{L}(\alpha|\theta) & = n \ln \Gamma(\alpha m_0) - nm_0\ln\Gamma(\alpha) + \sum_{ij}(\alpha-1)\ln \theta_{ij}\nonumber\\
\frac{d}{d\alpha}\ln \mathscr{L}(\alpha|\theta) &= nm_0\psi(\alpha m_0) - nm_0\psi(\alpha) + \sum_{ij} \ln \theta_{ij}
\end{align}
where $\psi(x)$ is the digamma function, $\psi(x) = \frac{d}{dx}\ln\Gamma(x)$.
Setting this to zero, we find that the MLE of $\alpha$ can be obtained empirically from the sum of the log probabilities $\theta_{ij}$.
\begin{align}
0 &= nm_0\psi(\alpha m_0) - nm_0\psi(\alpha) + \sum_{ij} \ln \theta_{ij} \nonumber\\
nm_0\psi(\alpha) - nm_0\psi(\alpha m_0) &= \sum_{ij} \ln \theta_{ij}\nonumber\\
\psi(\alpha) - \psi(\alpha m_0) & = \frac{1}{nm_0}\sum_{ij} \ln \theta_{ij} \label{eqn:mle}
\end{align}
% end modified from heike/lineup scenarios

The second derivative of the log likelihood function uses the trigamma function, $\psi_1(x) = \frac{d^2}{dx^2} \ln \Gamma(x)$. $\psi_1(x)$ can also be written as the series $\psi_1(x) = \sum_{z=0}^\infty \frac{1}{(z + x)^2}$. Thus,
\begin{align}
\frac{d^2}{d\alpha^2}\ln \mathscr{L}(\alpha|\theta) &= nm_0^2\psi_1(\alpha m_0) - nm_0\psi_1(\alpha)\\
& = nm_0^2\left(\sum_{x=0}^\infty \frac{1}{(\alpha m_0 + x)^2} - \sum_{x=0}^\infty \frac{1}{m_0(\alpha + x)^2}\right)\nonumber\\
& = nm_0^2\sum_{x=0}^\infty \left(\frac{1}{(\alpha m_0 + x)^2} - \frac{1}{m_0(\alpha + x)^2}\right) \nonumber\\
& = nm_0^2\sum_{x=0}^\infty \frac{m_0(\alpha + x)^2 - (m_0\alpha + x)^2}{m_0(\alpha m_0 + 1)^2(\alpha + x)^2}\leq 0 \text{ for } m_0 \geq 1, \alpha > 0 \nonumber\\
& \hphantom{=} \text{ as } m_0(\alpha+x)^2 \leq (\alpha m_0 + x)^2 \text{ for } m_0 \geq 1, \alpha > 0\nonumber
\end{align}
The empirical solution to \autoref{eqn:mle} is thus a local maximum.

\todo{Probably need to derive the std error... but that's likely to be trickier with the empirical solution? Need to check 601 notes or something. That's going to get more complicated if we show the weighted version... fml. I've only done all of this derivation for the simple unweighted version.}
% stolen from heike/lineup-scenarios
<<alpha-ml, include = F>>=
source("code/alpha_ml.R")
@

% end stolen from heike/lineups scenarios
Thus, the MLE of $\alpha$ is the numerical solution to the equation \autoref{eqn:mle}.

\subsection{Pooling null evaluations}
The MLE for $\alpha$ in \autoref{eqn:mle} allows for the combination of evaluations of multiple sets of null plots (either from Rorshach lineups or lineups with one or more target panels), but in order for this combination of data to be meaningful, the following conditions should hold:
\begin{enumerate}
\item Null plots should be generated by the same model
\item Only plots with the same aesthetics should be pooled for $\alpha$ estimation
\item Selection method should be the same (e.g. single or multiple target plots)
\end{enumerate}

When estimating $\hat\alpha$ from null plot selections in a standard lineup (e.g. a lineup containing at least one target plot), there is the possibility that no null plots are selected. If this is a frequent occurrence, it may be necessary to pool across aesthetics or null model parameter sets in order to obtain reasonable $\hat\alpha$s. A more reliable and systematic way to estimate $\hat\alpha$ would be to include a Rorshach lineup for each set of parameters used to generate null plots. These Rorshach lineups could be integrated into the testing procedure, or could be part of a pilot study used to assess the null plot generating model before it is used in lineups with data targets.

% \todo{default of ?? if no null plots are selected.} \svp{I know we talked about the default, I just don't remember what it was... as sum(log theta) -> - infinity, alpha -> 0 from what I can tell empirically. }

Intuitively, $\alpha$ is related to the proportion of null plots which would have some visually interesting feature: if $\alpha$ is low, that is, $\alpha << 1$, $\theta$s generated by the model would tend to be close to zero, with one or two larger panel selection probabilities (that is, one or two of the panels would be significantly more noticable). If $\alpha$ is high, $\alpha >> 1$, $\theta$s generated by the model would be closer to $1/m_{0}$, that is, each panel would be approximately equally likely to be selected. We know that the data generating model is likely to affect $\alpha$, and from \citet{vanderplas:2017} we know that the aesthetics can also significantly effect the selection of panels in a lineup.

In order to illustrate the variability in $\hat\alpha$ estimates across different lineup studies and to get a sense of the range of reasonable $\hat\alpha$ values, we estimated $\alpha$ for several previous single-target lineup studies of various sizes and designs. In several of these studies, multiple panel selections were allowed; these are allocated as partial selections to each set of counts.


<<null-alpha-single-target, fig.cap = "Alpha estimates for several lineup studies.", fig.width = 8, fig.height = 6, warning = F, message = F>>=
plot_df %>%
  ggplot(aes(x = dataset2, y = alpha, color = type, group = presentation)) +
  facet_wrap(~study, scales = "free_x", ncol = 4) +
  scale_color_manual("Panel Selection Type", values = c("orange4", "purple")) +
  geom_point(position = position_dodge(width = .3), shape = 1) +
  theme_bw() +
  theme(legend.position = "bottom") +
  theme(axis.title.x = element_blank()) +
  scale_y_continuous(expression(hat(alpha))) +
  ggtitle(expression(paste(hat(alpha), " for Single-Target Lineup Studies")), subtitle = "Estimated from 19 null plots")
@

\autoref{fig:null-alpha-single-target} shows the estimated $\alpha$s for each set of data, parameters, and aesthetics used in the studies. In most studies, $\hat\alpha \approx 0.07$ (5\% - 95\% quantiles: \Sexpr{sprintf("%0.3f and %0.3f", quantile(filter(plot_df, !study %in% c("Study 5", "Study 6"))$alpha, .05), quantile(filter(plot_df, !study %in% c("Study 5", "Study 6"))$alpha, .95))}), with very few estimated values over 0.10. There are some exceptions: Study 5 and Study 6 have $\hat\alpha$ values between \Sexpr{sprintf("%0.3f and %0.3f", min(filter(plot_df, study == "Study 5")$alpha), max(filter(plot_df, study == "Study 5")$alpha))}, which is a much wider range. In these studies, it also appears that plots with multiple selections allowed tend to have higher $\hat\alpha$ values, suggesting that multiple plot selections might allow for more diffusion of probabilities over multiple null plots. This effect might be heightened by increased lineup difficulty, which would also tend to increase $\hat\alpha$ values. One lineup from Study 5 is shown in \autoref{fig:hard-lineup}; only one participant identified the data target from among the nulls, suggesting that this is a difficult lineup, but the diffusion of identifications across many null panels indicates that the null data model generates plots with a relatively homogeneous level of visual interest.

\begin{figure}
\centering
\includegraphics[width=.6\textwidth]{lineup-images/turk11-filec2a72fe67dcf-multiple.pdf}
\caption{A lineup from Study 5. The target plot is in panel 7+7. Only one participant identified the target plot in 24 lineup evaluations. A total of 15 panels in this lineup were selected, indicating that the $\hat\alpha$ associated with this plot should be relatively large.}\label{fig:hard-lineup}
\end{figure}

\subsection{Variability in $\alpha$ as a diagnostic}

One issue with lineup experiments is that participants sometimes identify features in the null plots which are interesting, but not necessarily interesting in the same way as the feature under investigation that is shown in the data plot. In some cases, this is because the lineup is particularly challenging or the target plot's effect is not visually significant, but in others, it can be an indicator that the null plot generation model is not adequately reproducing features in the data. In large lineup experiments, it can be difficult to screen every panel of every lineup for systematic null set generation issues, resulting in participant selection of null plots which have unintentional visually interesting features, as in \citet{vanderplas:2017}. In that study, null panel data were clustered using k-means; this clustering occasionally produced clusters of 1-3 points or clusters which were oddly shaped, and the panels showing these data sets were frequently identified by participants even though the original intent of the experiment was to assess the visual salience of clustering and linear trends in the presence of different aesthetic combinations. In the case of clusters of one or two points, estimation of a 95\% bounding ellipse failed, providing an unintended feature which participants noticed, resulting in significant null plot identifications. \autoref{fig:featurehierarchy-null-model-sucks} shows two lineups which contain the same data; the clustering issues are much more noticable in the lineup with color, shape, and bounding ellipse aesthetics.


<<null-alpha-est>>=
source("code/turk16_null_alpha.R")
@

<<featurehierarchy-null-model-sucks, fig.width = 12, fig.height = 6, fig.cap = "A two-target lineup from \\citet{vanderplas:2017}, plotted with different aesthetic combinations. The two targets are in plot 11+8 (clustering) and 2+2 (linear trend). In plots 10, 11, 15, and  17, the estimation of a bounding ellipse in a null plot failed due to insufficient points in the cluster. This deficiency is less apparent in the second lineup, which only has a regression line, as the aesthetics which emphasize grouping are not shown.", warning = F, message=F>>=
source("code/turk16_plots.R")
turk16_lineup_dat <- read_csv("data/Turk16/Image_Data/set-46-k-5-sdline-0.45-sdgroup-0.25.csv")

turk16_lineup_alphas <- filter(t16_null_alpha_plotwise, set_number == 46, aes_type %in% c("trend", "colorShapeEllipse")) %>% arrange(aes_type)

gridExtra::grid.arrange(
  suppressWarnings(gen.plot(turk16_lineup_dat, c("Color", "Shape"), c("Ellipses"))) +
    coord_fixed() + scale_x_continuous(limits = c(-3, 2.75)) + scale_y_continuous(limits = c(-3, 2.75)),

  suppressWarnings(gen.plot(turk16_lineup_dat, NULL, "Reg. Line")) +
    coord_fixed() + scale_x_continuous(limits = c(-3, 2.75)) + scale_y_continuous(limits = c(-3, 2.75)),
  nrow = 1
)
@

One possible solution to this problem is to run a small pilot study in which a Rorshach lineup is used to examine the selection frequency of different null plots. If one or two null plots are systematically selected from this Rorshach lineup, or if the $\hat\alpha$ values are highly variable, then the data-generating model may need to be examined for unintended effects such as those in \citet{vanderplas:2017}. Alternately, $\hat\alpha$ can be estimated from the null plot selections of a standard lineup, though if the lineup is relatively easy and null plots are selected infrequently, these estimates may be unstable.

\todo{Conduct a null lineup study with the same parameters as the original, so that the estimates of $\alpha$ are comparable.}

It is reasonable to think that $\hat\alpha$ would be different for the two lineups in \autoref{fig:featurehierarchy-null-model-sucks} even though the underlying data is the same: the color, shape, and ellipse aesthetics emphasize the missing elements and small cluster size, while this deficiency is much less apparent in the second lineup, which contains no aesthetic cues to indicate data clustering. We can estimate plot-specific $\hat\alpha$ for each of the plots shown in \autoref{fig:featurehierarchy-null-model-sucks}; the values are \Sexpr{sprintf("%0.3f and %0.3f", turk16_lineup_alphas$alpha[1], turk16_lineup_alphas$alpha[2])}, respectively. Thus, in the null plots shown in the first figure, which has color, shape, and ellipse aesthetics, $\theta_i, i = 1, ..., m_0$, there is more diffusion of probability over several plots than in the second lineup, which has only a regression line added to the plot.

We can examine the $\hat\alpha$ estimates for each set of aesthetics to explore empirical support for this hypothesis. The full experiment details can be found in \citet{vanderplas:2017}; for the purposes of this reanalysis dealing only with the null plots, data was generated using three parameters: $k \in c(3, 5)$, the number of clusters, $\sigma_T$, the standard deviation around the trend line, and $\sigma_C$, the deviation around the cluster centers. Three values of $\sigma_T$ and $\sigma_C$ were used to represent varying degrees of difficulty, with small $\sigma$ values hypothesized to be easier than large $\sigma$ values. Thus, there are a total of 18 parameter combinations, and at each parameter combination there are 3 replicate lineup data sets, for a total of 54 datasets. Each dataset is rendered in 10 different aesthetic combinations, for a total of 540 plots.

With this design in mind, individual $\hat\alpha$ values were calculated from the null plot selection data for each lineup, to get a sense of the variability in the $\hat\alpha$ estimates. In some lineups, null plots were never selected; estimates have been excluded for these plots. \autoref{fig:all-null-alphas} shows the results by aesthetic for each lineup data set used in \citet{vanderplas:2017}.
% <<all-null-alphas, fig.cap = "Alpha estimates for each lineup image with at least 5 null panel selections, with aggregate estimates for each parameter set and aesthetic combination with a total of 5 null panel selections across multiple plots. There are some differences between aesthetics in average $\\hat\\alpha$, but overall, most $\\alpha$ values are estimated to be around 0.7. The variability in $\\hat\\alpha$ increases with the value of $\\sigma_C$ and $\\sigma_T$, the trend and cluster variability parameters. Note that on the y-axis, aesthetics are indicated as follows: C = color, L = ellipse, S = shape, T = trend line, E = error band. $\\hat\\alpha$s tend to be more variable when color aesthetics are present - if only one null plot has an unequal group, $\\alpha$ will tend to be very small; if more than one null plots have unequal groups, $\\alpha$ tends to be larger. Pooled estimates of the 3 plots generated with the same parameters and displayed with the same aesthetics are shown with an 'x'.", fig.width = 8, fig.height = 6>>=
%
% plot_t16_alpha_df <- t16_null_alpha_plotwise %>%
%   mutate(valtype = "Individual Plot") %>%
%   bind_rows(
%     mutate(t16_null_alpha_setwise, valtype = "Pooled")
%   ) %>%
%   mutate(label = sprintf("Plots %02.0f-%02.0f", set_min, set_min+2),
%          klab = sprintf("k == %d", k),
%          siglab1 = paste("sigma[T] == ", round(sdline, 2)),
%          siglab2 = paste("sigma[C] == ", round(sdgroup, 2)),
%          aes_type = fix_plot_names(aes_type)) %>%
%   ungroup() %>%
%   mutate(sig_int = interaction(sdline, sdgroup) %>% factor() %>% as.numeric()) %>%
%   group_by(k) %>%
%   mutate(y_val = (sig_int - 6)/9) %>%
%   ungroup()
%
% plot_t16_alpha_segs <- plot_t16_alpha_df %>%
%   group_by(aes_type, siglab1, siglab2, sig_int) %>%
%   summarize(minp = min(alpha), maxp = max(alpha), y_val = unique(y_val))
%
% ggplot() + coord_flip() +
%   scale_size_continuous("# Null Evaluations", range = c(1, 4)) +
%   geom_segment(aes(x = y_val, xend = y_val, y = minp, yend = maxp),
%                data = plot_t16_alpha_segs,
%                alpha = .25) +
%   geom_point(aes(y = alpha, x = y_val, size = n_total,
%                  color = factor(k),
%                  shape = valtype),
%              data = plot_t16_alpha_df) +
%   scale_shape_manual("Estimate Type", values = c("Individual Plot" = 1, "Pooled" = 4)) +
%   scale_color_manual("# Clusters", values = c("3" = "purple", "5" = "orange")) +
%   ylab("") +
%   scale_y_continuous(name = expression(alpha)) +
%   scale_x_continuous(breaks = NULL) +
%   # facet_grid(siglab1 ~., labeller = label_parsed) +
%   facet_grid(aes_type ~ .) +
%   theme_bw() +
%   theme(legend.position = "bottom", axis.title.y = element_blank())
% @
%
% <<all-null-alpha-pooled, fig.cap = "Alpha estimates calculated by pooling within null plot generating parameters and between null plot generating parameters. It is clear that the introduction of color and other aesthetics which highlight clustering causes an increase in $\\hat\\alpha$ estimates when data is aggregated; this effect is likely due to the inclusion of plots with several null targets with groups of noticably different sizes; there is significant variability in the $\\hat\\alpha$ estimates based on single data sets and based on single parameter sets because the randomly generated null plot data does not consistently produce the same number of null plots with this interesting characteristic.", fig.width = 4, fig.height = 5>>=
% t16_null_alpha_setwise %>%
%   mutate(valtype = "Pooled (single parameter set)") %>%
%   bind_rows(
%     mutate(t16_null_alpha_aeswise, valtype = "Pooled (all parameter sets)")
%   ) %>%
%   mutate(label = sprintf("Plots %02.0f-%02.0f", set_min, set_min+2),
%          klab = sprintf("k == %d", k),
%          siglab1 = paste("sigma[T] == ", round(sdline, 2)),
%          siglab2 = paste("sigma[C] == ", round(sdgroup, 2)),
%          aes_type = fix_plot_names(aes_type)) %>%
% ggplot(aes(x = alpha, y = aes_type)) +
%   geom_point(aes(shape = valtype, alpha = valtype)) +
%   scale_shape_manual("Estimate Type", values = c("Pooled (single parameter set)" = 4, "Pooled (all parameter sets)" = 20)) +
%   scale_alpha_manual("Estimate Type", values = c("Pooled (single parameter set)" = .5, "Pooled (all parameter sets)" = 1)) +
%   ylab("") +
%   scale_x_continuous(name = expression(alpha)) +
%   theme_bw() +
%   theme(legend.position = "bottom", axis.title.y = element_blank())
% @

The estimated $\hat\alpha$ value can then be used either in the specification of the null model $\alpha$ ($\alpha_{M_2}$) or in the calculation of the visual p-value under the frequentist framework. Currently, in either scenario, the data used in the calculation are the target plot identifications and the sum of all null plot identifications; the individual values of the null plot identifications are not used. These data can be reclaimed to estimate a general $\alpha$ for the overall null model generation scheme, or, in an ideal situation, Rorshach lineups could be used to estimate $\alpha$ directly without any possible contamination effects induced by the presence of target plots.

\section{Impact of $\hat\alpha$ Selection}



\bibliographystyle{asa}
\bibliography{references}




\end{document}

