\documentclass[12pt]{article}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{multirow}
\usepackage{hyperref}
% \usepackage{endfloat} % Figures to the end of the document

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
%---------------------------------------------------
%                 Editing Commands
\newcommand{\hh}[1]{{\color{magenta} #1}}
\newcommand{\svp}[1]{{\color{darkgray} #1}}
\newcommand{\fix}[1]{{\color{blue} #1}}
\newcommand{\todo}[1]{{\color{purple} #1}}

%---------------------------------------------------
%                 Placing Figures
\usepackage[font=small,skip=5pt]{caption}
\usepackage{subcaption}
\usepackage{afterpage}
\usepackage{rotating}
\usepackage[dvipsnames,svgnames,table]{xcolor}
\graphicspath{{figure/}}

%---------------------------------------------------
% Define new environment
\newtheorem{theorem}{Theorem}[section]
\newtheorem{algorithm}[theorem]{Algorithm}
%---------------------------------------------------

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%


\begin{document}

% \bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}{#1}\small\normalsize}
\spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind {
  \title{\bf A Bayesian approach to visual inference}
  \author{Susan VanderPlas, Heike Hofmann\thanks{
    The authors gratefully acknowledge funding from the National Science Foundation Grant \# DMS 1007697. All data collection has been conducted with approval from the Institutional Review Board IRB 10-347}\hspace{.2cm}\\
    Department of Statistics and Statistical Laboratory, Iowa State University}%
  \maketitle%
} \fi

\if1\blind {
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf A Bayesian approach to visual inference}
\end{center}
  \medskip
} \fi
<<setup, echo = F, include = F>>=
library(tidyverse)
knitr::opts_chunk$set(echo = F, message = F, warning = F)
# theme_set(theme_bw())
@

<<code, include = F>>=
source("code/functions.R")
@

<<turk-studies-data,  include = F>>=
source("code/process_all_lineup_data.R")
@

\section{Introduction}

% Introduction to graphical testing: Rorschach vs. Visual Inference

% Graphical Testing vs. Statistical Testing - power improvements w/ additional people

% Something about the power of the human visual system? Don't want to get too off topic...

Graphics provide the opportunity to understand statistical data at an intuitive level: we can gain more information about the relationship between two variables by considering a simple scatter plot than we might obtain from an entire day of generating numerical summaries. Graphics leverage the bandwidth of the visual system for implicit data analysis, and because this analysis is implicit, we often assume graphics are not decisive in the same way that an hypothesis test is decisive: generally, graphics do not come with a significance threshold, and in many cases, we do not explicitly construct the hypothesis we might be testing before viewing the chart.

Visual inference allows for us to test graphics as visual statistics: charts are, after all, a quantity computed from values in a sample of data. In order to test whether a chart shows a visually significant result, we can use the same philosophy used by randomization tests: construct a sampling method consistent with the null hypothesis, generate many copies of the test statistic (in this case, the plot), and see where the real statistic falls in the distribution of artificially generated quantites \citep{buja:2009}. An assembly of several null plots with a target (or data) plot is called a \emph{lineup}, after the criminal procedure of the same name. Typically, lineups are composed of 19 ``null" plots (generated under the null hypothesis) and one data plot containing the real data.

Of course, with numerical statistics, there is a natural ordering to computed numerical quantities; with plots, we must run each statistic through another process (evaluation by the visual system, or a facsimile thereof\footnote{Cite Giora's Deep Learning work, \url{http://giorasimchoni.com/deep_visual_inference.html}}) in order to evaluate significance. During this evaluation process, the user selects one or more plots from the lineup which are ``different" in some way (though some experiments may specify the particular feature under examination).

% TODO: explain different lineup scenarios here? Not sure if this is the best place for it, or if it's necessary at all.

Typically, graphical tests utilize a service like Amazon Mechanical Turk to acquire multiple evaluations of the same lineup; informally, if multiple individuals select the plot generated from the data rather than the null plots, the visual statistic is likely to be significant \citep{majumder2013validation}.

% TODO: add lineup here

While visual inference was initially developed to mimic frequentist hypothesis tests, using lineups of 20 plots with one data plot, so that the probability of selecting the data plot randomly is $p=0.05$, visual inference itself does not demand use of frequentist techniques. In this paper, we develop one possible framework for visual inference, using a Bayesian framework with a Dirichlet-multinomial distribution to model the probabilities of selecting each panel and a Multinomial distribution to model the observed participant selections. This framework for analysis of visual inference data has been in use as part of the \texttt{vinference} package~\citep{vinference} for several years \citep{loyAreYouNormal2015,loy2016variations,vanderplas:2017}, but has not been formally described in any publication. Here, we provide the mathematical foundation for the multinomial-dirichlet model used in the analysis of visual inference experiments, exploring the implications of the model and proposing a modification which better describes the perceptual process of lineup evaluation.

\subsection{Lineup Evaluations}\label{sec:scenarios}
% Types of lineups
\citet{buja:2009} introduced two types of lineups: Rorshach lineups, which contain only null plots, and standard one-target lineups, which contain $m$ panels, $m-1$ of which are null plots, and one which shows the real data. \citet{vanderplas:2017} introduced another type of lineup: the two-target lineup. This lineup contains target plots generated from two competing data generating models, with null plots that are generated from a mixture of the two target plot distributions. Two-target lineups can be used to test whether two competing effects have different visual salience. All of the lineup experiments the authors are aware of at this time are one or two-target lineup experiments.

% Types of lineup evaluations
In addition to different types of lineups, there are different types of lineup experiments.

\begin{description}
\item [Scenario 1] $K$ different lineups are shown to $K$ independent individuals. In this scenario, both the data and the null plots in each generated lineup are distinct from those in every other lineup. This scenario is only practical when using purely simulated data (for both the data and null plots) or data large enough to allow for subsampling to generate $K$ different data plots. Under this scenario, we can consider the number of target plot selections $t$ out of $K$ total evaluations, where each lineup evaluation is a bernoulli trial; the total number of data plot evaluations can then be modeled as a Binomial distribution with selection probability $1/m$ (for a single target lineup) \citep{majumder2013validation}.

\item [Scenario 2] $K$ different sets of null plots are shown to $K$ independent individuals; the same data plot is used in each lineup. Alternately, $L$ sets of lineups are shown to $K > L$ individuals. In Scenario 2, there are dependencies introduced by reuse of the data or the lineups, providing an intermediate case between the two extremes of Scenario 1 and Scenario 3.

\item [Scenario 3] The same lineup is shown to $K$ independent individuals. This scenario is the most common scenario in the lineup experiments which have been completed to date \citep{hofmann2012graphical,roychowdhury:2012,majumder2013validation,loyAreYouNormal2015,vanderplas:2017}. In this scenario, lineup evaluations by independent viewers are not independent because the viewers are evaluating the same combination of 19 null plots and one data plot. Any peculiar features which arise in a null plot may cause participants to select that null plot over the data plot; it is likely that where one individual makes this choice, others might as well. Thus, in Scenario 3, which is the most common lineup experiment scenario, it is not reasonable to assume that all panels are equally likely to be selected under the null hypothesis.
\end{description}

Scenario 3 is the primary motivation for the model which we will develop in the next section: while we cannot consider the panels in each lineup as equally likely to be selected, we can model the individual selection probabilities using a hierarchical model.

\section{Lineup Model Specification}

We will begin with a generic $m$-panel lineup, with selection probabilities $\theta_i, i = 1, ..., m$ where $\sum_{i=1}^m \theta_i = 1$, that is, the participant will select one (and only one) panel from the lineup as the most different. Our lineup has been evaluated by $K$ individuals, with $c_i, i = 1, ..., m$ the selection count for each panel, and $K = \sum_{i=1}^m c_i$.

A natural data model for this data is the Multinomial distribution, which has parameters $K, \bm{\theta}$, where $K$ describes the number of distinct outcomes and $\bm{\theta} = \theta_1, ..., \theta_m$ describes the probabilities of each outcome. We will fix $K$, as that is controlled by the experimental design, and model $\bm{\theta}$.

\begin{align}\label{eqn:multinomial-pmf}
f(\bm{c}|K, \bm{\theta}) & = \frac{K!}{c_1! \cdots c_m!} \prod_{i=1}^m \theta_i^{c_i}
\end{align}

We model panel selection probabilities $\theta_i$ using a Dirichlet distribution with concentration hyperparameter $\bm{\alpha}$, which happens to be conjugate to the multinomial distribution. As the position of the panels within the lineup are random, we use a symmetric Dirichlet distribution, with $\alpha_i = \alpha, i = 1, ..., m$, that is, the concentration hyperparameter is constant. This allows us to vary the lineup difficulty through the hyperparameter $\alpha$ without having to specify which plot $i$ is the target plot.

The pdf of the symmetric Dirichlet distribution is
\begin{align}\label{eqn:dirichlet-pdf}
f(\bm{\theta}|\alpha) & = \frac{\left(\Gamma(\alpha)\right)^m}{\Gamma(m\alpha)} \prod_{i=1}^m \theta_i^{\alpha - 1}
\end{align}

Using the conjugate relationship between the Dirichlet and Multinomial distributions, we then get the posterior distribution as the Dirichlet$(\bm{c + \alpha})$ distribution.

\begin{align}\begin{split}\label{eqn:full-model-specification}
(\alpha_1, ..., \alpha_m) = \bm{\alpha} &= \text{concentration hyperparameter}\\
(c_1, ..., c_m) = \bm{c} &= \text{observed plot selections}, \sum_{i=1}^m c_i = K\\
p(\bm{\theta}) &\sim Dirichlet(\bm\alpha) \\
f(\bm{c} | \bm{\theta}) & \sim Multinomial(\bm\theta, K)\\
p(\bm\theta | \bm{c}, \bm{\alpha}) & \sim Dirichlet(\bm{c} + \bm{\alpha})
\end{split}\end{align}

where $Multinomial(\bm\theta, K)$ is defined as in \autoref{eqn:multinomial-pmf} and $Dirichlet(\bm\alpha)$ is defined as in \autoref{eqn:dirichlet-pdf}.

Typically, when evaluating lineups, we compare the number of target plot identifications with the aggregate number of null plot identifications (see \citet{majumder2013validation}). This is equivalent to the marginal distribution of $c_t$, where $t \in 1, ..., m$ is the index of the target panel in the lineup; that is, in \citet{majumder2013validation}, a binomial distribution, and in this formulation, a beta-binomial distribution.

\begin{align}\begin{split}\label{eqn:marginal-model-specification}
\alpha &= \text{concentration hyperparameter}\\
c_t &= \text{target plot selections},\\
K &= \text{total evaluations}\\
p(\theta_t) &\sim Beta(\alpha, (m-1)\alpha) \\
f(c_t | \theta_t) & \sim Binomial(\theta_t, K)\\
p(\theta_t | \bm{c_t}, \alpha) & \sim Beta(c_t + \alpha, K - c_t + (m-1)\alpha)
\end{split}\end{align}

While this model was originally developed under a Bayesian framework, it is philosophically agnostic: it would be equally reasonable to think of this as an overdispersed multinomial model.

Examining the parameters of either the full or marginal model specifications in \autoref{eqn:full-model-specification} and \autoref{eqn:marginal-model-specification}, it is evident that $\alpha$ provides the equivalent of pseudo-observations for each plot; that is, the effect of $\alpha$ is equivalent to adding $\alpha$ identifications to each panel in the lineup. When $\alpha$ is small, these pseudo-observations have relatively little influence, but when $\alpha$ is large, the pseudo-observations can quickly dwarf any information provided by the data. This is particularly true for the marginal Beta-Binomial model, where the equivalent of $(m-1)\alpha$ pseudo-observations are added. In most lineup studies, a plot might be evaluated between 10 and 30 times; with a $m=20$ lineup, even $\alpha = 1$ can easily dominate the participant selection data.

In addition to the pseudo-observation interpretation, $\alpha$ provides information about the number of panels in a lineup which are likely to attract participant interest. It is useful to detour slightly from the discussion of visual inference to explore the impact and interpretation(s) of $\alpha$ in the context of statistical lineups.

\subsection{Dirichlet Hyperparameter}\label{sec:alpha}
% Stolen from heike/lineup-scenarios
When $\alpha = 1$, the symmetric Dirichlet distribution is uniform on the $m-1$ dimensional simplex. When $\alpha < 1$, the mass of the distribution is along the edges of the simplex, where most values of $\theta_i$ will be close to 0. When $\alpha>1$, the mass of the distribution is in the center of the simplex, with most of the $\theta_i$ having similar values. \autoref{fig:simplex} shows ternary plots~\citep{ggtern} of values simulated from a 3-dimensional Dirichlet distribution which illustrate the effect of $\alpha$ on the sampled $\bm\theta$.

<<simplex, echo=FALSE, out.width='\\textwidth', fig.width=12, fig.height=4, fig.cap = "Dirichlet distributed samples on the 2-dimensional simplex.">>=
simplex <- purrr::map_df(.x = c(1/3, 1, 3), ~data.frame(
  gtools::rdirichlet(5000, alpha = rep(.x, 3))
), .id = "alpha") %>%
  rename(p1 = X1, p2 = X2, p3 = X3) %>%
  mutate(alphalabel = factor(alpha, levels = 1:3, labels = c("alpha: 1/3", "alpha: 1", "alpha: 3")))

ggtern::ggtern(aes(x = p1, y = p2, z = p3), data = simplex) +
  geom_point(alpha = .075, shape = 16) +
  facet_wrap(~alphalabel, labeller = "label_parsed") +
  theme(panel.background = element_rect(colour = "black"))
@
% End stolen from heike/lineup-scenarios

<<set-theme-bw, include = F>>=
theme_set(theme_bw())
@

While graphical illustrations of the 20-dimensional dirichlet distribution are more difficult, we can use prior predictive simulations to assess the meaning of $\alpha$ as it relates to how many panels in a lineup attract participant attention. \autoref{fig:prior-predictive} shows simulated $c_i$ counts (sorted for visual clarity) for several values of $\alpha$; it is evident that for $\alpha<0.05$ only one panel of the lineup receives significant attention, while for $\alpha> .25$, participant attention is divided among several interesting panels of the lineup.

<<prior-predictive, echo = F, fig.cap = "Prior predictive distribution of number of participant selections of panels $c_i$ (sorted by frequency) for different values of $\\alpha$, with fixed $K=20$ plot evaluations. Low values of $\\alpha$ have fewer plots with any participant selections, while higher values of $\\alpha$ have more plots with participant selections.", fig.width = 8, fig.height = 4, out.width = "\\textwidth" >>=

sim_lineup_model <- function(alpha, m = 20, k = 20, N = 50) {
  theta <- gtools::rdirichlet(1, rep(alpha, m))
  sels <- rmultinom(N, size = k, prob = theta)
  sels
}

alphas <- c(.001, .02, .05, .1, .5, 1, 2, 5, 20, 1000)
prior_pred <- tibble(alpha = alphas,
                     plot_sels = purrr::map(alpha, sim_lineup_model, N = 100)) %>%
  mutate(
    sel_ordered = purrr::map(plot_sels, ~apply(., 2, sort, decreasing = T)),
    sel_ordered_long = purrr::map(
      sel_ordered,
      ~tibble(idx = rep(1:nrow(.x), times = ncol(.x)),
              rep = rep(1:ncol(.x), each = nrow(.x)),
              sels = as.vector(.x, mode = "numeric")))
  ) %>%
  select(-plot_sels, -sel_ordered) %>%
  unnest() %>%
  arrange(alpha) %>%
  mutate(label = sprintf("alpha == %f", alpha) %>% factor(levels = sprintf("alpha == %f", alphas), ordered = T))

ggplot(prior_pred) +
  geom_path(aes(x = idx, y = jitter(sels, amount = .4), group = interaction(rep, alpha)), alpha = .05) +
  facet_wrap(~label, labeller = label_parsed, nrow = 2) +
  scale_x_continuous("Ordered panel number") +
  scale_y_continuous("# Simulated Panel Selections (of 20 evaluations)")

@

% A modification of the frequentist paradigm developed in \citet{majumder2013validation} and released as a technical report \todo{get citation!} simulates p-values from a Dirichlet-Multinomial distribution with $\alpha=1$; based on the prior predictive distributions alone, $\alpha=1$ does not appear to fit the observed data or theoretical process (e.g. the tendency to identify even small differences from among similar things) well.

The data model used in \citet{majumder2013validation} assumes $\theta_t = 1/m$, that is, $\theta_t$ is fixed and equal to the selection probability of every other panel in the lineup. This assumption, which would correspond to infinite $\alpha$, does not match our experience when evaluating a lineup, nor the accumulated experimental evidence (assembled under Scenario 3, as discussed in \autoref{sec:scenarios}) which shows that even null plots do not show equal selection probabilities for each panel. When examining a lineup, we are generally drawn immediately to 1-4 panels, and the remaining evaluation is to decide between those panels; we also know that typically the same panels are selected across multiple individuals. To account for this issue, recent analyses of lineups calculate visual p-values using the mass function
\begin{align}\label{eqn:beta-binomial}
P(C\geq x) = \sum_{x = C}^{K} \binom{K}{x} \frac{1}{B(\alpha, (m-1)\alpha)}\cdot B(x+\alpha, K-x+(m-1)\alpha)
\end{align}
where $C$ is the number of data panel detections and $K$ is the number of independent evaluations of the lineup~\citep{vinference,vanderplas:2017,loyAreYouNormal2015}. A derivation of this mass function is provided in the appendix. A similar method is found in the \texttt{vinference} package, which calculates visual p-values by simulating draws of $\theta$ from a uniform distribution (corresponding to the assumption that $\alpha=1$, as shown in \autoref{fig:simplex}) but the more general solution is useful to consider, as we may not actually believe $\theta$ is uniformly distributed over the $(m - 1)$ simplex.

\subsection{Hyperparameter Selection}
% As a result, it's important to understand the effect of $\alpha$ on the posterior distribution.

We know from experience as well as cognitive principles that it is unreasonable to assume that the selection probability of every null plot is precisely equal: null plots are randomly generated, and occasionally, the randomly generated plot will have an interesting feature (that may or may not be present in the target plot).
When that occurs, the interesting null plot will be selected more frequently than the other nulls, despite being generated by the same distribution.
The ability to identify stimuli as being different from one another is a fundamental part of cognition; the abstractions that allow us to use the terms `same' and `different' are fundamental to human intelligence~\citep{mingWhenThingsAre2017}.
As a result, when presented with a lineup, we will typically gravitate towards one or two panels which are different from our mental representation of a generalized panel on some measure, though not always the measure that's under investigation.
Note that even though the interesting null plot effect is found in all scenarios proposed in \autoref{sec:scenarios}, it is not a systematic issue in Scenario 1; in Scenario 3, the null plots are not re-generated with each lineup evaluation, so an interesting null plot selected by one individual may also effect the evaluation of by other individuals.
That is, in Scenario 3, the interesting null plots affect all evaluations, and this effect can significantly affect the experiment results.
However, in Scenario 3, we can also estimate the size of this effect: by examining repeated evaluations of a lineup, we can leverage that replication to estimate the distinctiveness of the set of null plots in the lineup.
While this effect may exist in Scenario 1, we cannot estimate the effect because there are not repeated evaluations of the same set of lineups. Thus, in Scenario 1, the best estimate we can make for the generic $\theta$ is $1/m$, which is equivalent to the Multinomial model without the Dirichlet hyperparameter and with $\theta = 1/m$.

In the theoretical Multinomial-Dirichlet model proposed in this paper, the number of panels which could be expected to be visually different in a generalized lineup is a function of $\alpha$, the hyperparameter in \autoref{eqn:dirichlet-pdf} and \autoref{eqn:full-model-specification}.
In practice, we would create a null plot generating model first, and set $\alpha$ according to the perceived difficulty of lineup evaluation using the null generating model in question.
A difficult lineup, with many potentially interesting panels, would have a higher $\alpha$ value than an easy lineup with no null panels which were visually salient relative to the data panel.

$\alpha$ plays a similar role in modulating the visual p-value calculated from \autoref{eqn:beta-binomial}: lower $\alpha$ values produce a higher visual p-value estimate, and higher $\alpha$ values produce a lower visual p-value estimate.

<<vis-p-val-sensitivity-initial, fig.cap = "Sensitivity of visual p-value to selection of $\\alpha$ under the beta-binomial model. Corresponding values for the binomial model are shown on the right side of the plot; as $\\alpha \\rightarrow\\infty$, the beta-binomial p-values converge to the binomial model p-value.", fig.width = 8, fig.height = 5, out.width = "\\textwidth">>=
alphas <- exp(seq(-6, 6, by = .01))
data_breaks <- c(1:5, 6, 8, 10, 15, 20)


pv <- tidyr::crossing(alpha = alphas, C = data_breaks, K = 20) %>%
  mutate(p = vis_p_value(C, K, alpha))
pv2 <- tidyr::crossing(C = data_breaks, K = 20) %>%
  mutate(p = purrr::map2_dbl(C - 1, K, pbinom, prob = 1/20, lower.tail = F))

ggplot(pv, aes(x = alpha, y = p, color = factor(C), group = factor(C))) +
  geom_line(size = 1) +
  geom_point(aes(x = exp(6.25), y = p, color = factor(C), shape = "Binomial\np-value"), data = pv2) +
  scale_y_continuous("Visual p-value") +
  scale_x_continuous(expression(alpha), trans = "log10", breaks = c(0.001, 0.01, .1, 1, 10, 100),
                     labels = c("0.001", "0.01", "0.1", "1", "10", "100")) +
  scale_color_brewer("# Data\nPanel\nIdentifications\n(K = 20)", palette = "Paired") +
  scale_shape_discrete("") +
  geom_hline(yintercept = 0.05, color = "grey") +
  guides(color = guide_legend(override.aes = list(shape = NA))) +
  annotate("segment", x = exp(6.25), xend = exp(6.25), y = -Inf, yend = .7, color = "grey", alpha = .5) +
  annotate("text", x = exp(6.25), y = 0.75,
           label = "Binomial\nmodel", vjust = 1, hjust = .75) +
  annotate("segment", x = 1, xend = 1, y = -Inf, yend = .7, color = "grey", alpha = .5) +
  annotate("text", x = 1, y = 0.75,
           label = "vinference\npackage", vjust = 1, hjust = .5) +
  theme_bw()
@

Clearly, the choice of $\alpha$ is critical. From a practical perspective, the number of plots which are visually salient and thus likely to be selected by participants is a factor of the lineup design (zero, one, or two targets), null plot generation method, and possibly the form of the plot (aesthetics, geometric representations, scales). While the prior predictive distributions in \autoref{fig:prior-predictive} are illustrative, in practice, we do not usually have a good instinct for what a reasonable value of $\alpha$ would be for a particular lineup generation method. In the next section, we present a method for estimating $\alpha$ from Rorshach or standard lineups and discuss possible uses for this method in assessing null plot generation and lineup difficulty.

\section[Estimating alpha]{Estimating $\alpha$}
\subsection{Derivation of MLE for $\alpha$}
% modified from heike/lineup-scenarios
In order to estimate $\alpha$ from the null plots of a lineup, let $m_0$ be the number of null plots in an $m$- panel lineup. If the lineup is a Rorshach lineup, then $m=m_0$. We can estimate $\alpha$ using a set of such lineups, $j=1, ..., n$, where the lineups in the set were generated under the same distribution. The likelihood function is then
\begin{align}
\mathscr{L}(\alpha|\theta) &= \prod_{j=1}^n \left(\frac{1}{B(\alpha)}\right)^{m_0} \prod_{i=1}^{m_0} \theta_{ij}^{\alpha - 1}\nonumber\\
& = \left(\frac{\Gamma(\alpha m_0)}{\left(\Gamma(\alpha)\right)^{m_0}}\right)^n \prod_{ij} \theta^{\alpha-1}_{ij}
\end{align}
and the derivative of the log-likelihood function can be calculated as
\begin{align}
\ln \mathscr{L}(\alpha|\theta) & = n \ln \Gamma(\alpha m_0) - nm_0\ln\Gamma(\alpha) + \sum_{ij}(\alpha-1)\ln \theta_{ij}\nonumber\\
\frac{d}{d\alpha}\ln \mathscr{L}(\alpha|\theta) &= nm_0\psi(\alpha m_0) - nm_0\psi(\alpha) + \sum_{ij} \ln \theta_{ij}
\end{align}
where $\psi(x)$ is the digamma function, $\psi(x) = \frac{d}{dx}\ln\Gamma(x)$.
Setting this to zero, we find that the MLE of $\alpha$ can be obtained empirically from the sum of the log probabilities $\theta_{ij}$.
\begin{align}
0 &= nm_0\psi(\alpha m_0) - nm_0\psi(\alpha) + \sum_{ij} \ln \theta_{ij} \nonumber\\
nm_0\psi(\alpha) - nm_0\psi(\alpha m_0) &= \sum_{ij} \ln \theta_{ij}\nonumber\\
\psi(\alpha) - \psi(\alpha m_0) & = \frac{1}{nm_0}\sum_{ij} \ln \theta_{ij} \label{eqn:mle}
\end{align}
% end modified from heike/lineup scenarios

The second derivative of the log likelihood function uses the trigamma function, $\psi_1(x) = \frac{d^2}{dx^2} \ln \Gamma(x)$. $\psi_1(x)$ can also be written as the series $\psi_1(x) = \sum_{z=0}^\infty \frac{1}{(z + x)^2}$. Thus,
\begin{align}
\frac{d^2}{d\alpha^2}\ln \mathscr{L}(\alpha|\theta) &= nm_0^2\psi_1(\alpha m_0) - nm_0\psi_1(\alpha)\\
& = nm_0^2\left(\sum_{x=0}^\infty \frac{1}{(\alpha m_0 + x)^2} - \sum_{x=0}^\infty \frac{1}{m_0(\alpha + x)^2}\right)\nonumber\\
& = nm_0^2\sum_{x=0}^\infty \left(\frac{1}{(\alpha m_0 + x)^2} - \frac{1}{m_0(\alpha + x)^2}\right) \nonumber\\
& = nm_0^2\sum_{x=0}^\infty \frac{m_0(\alpha + x)^2 - (m_0\alpha + x)^2}{m_0(\alpha m_0 + 1)^2(\alpha + x)^2}\leq 0 \text{ for } m_0 \geq 1, \alpha > 0 \nonumber\\
& \hphantom{=} \text{ as } m_0(\alpha+x)^2 \leq (\alpha m_0 + x)^2 \text{ for } m_0 \geq 1, \alpha > 0\nonumber
\end{align}
The empirical solution to \autoref{eqn:mle} is thus a local maximum, and the MLE of $\alpha$ is the numerical solution to the equation \autoref{eqn:mle}.


% \todo{Probably need to derive the std error... but that's likely to be trickier with the empirical solution? Need to check 601 notes or something. That's going to get more complicated if we show the weighted version... fml. I've only done all of this derivation for the simple unweighted version.}
% stolen from heike/lineup-scenarios
<<alpha-ml, include = F>>=
source("code/alpha_ml.R")
@

% end stolen from heike/lineups scenarios

\subsection{Pooling null evaluations}
The MLE for $\alpha$ in \autoref{eqn:mle} allows for the combination of evaluations of multiple sets of null plots (either from Rorshach lineups or lineups with one or more target panels), but in order for this combination of data to be meaningful, the following conditions should hold:
\begin{enumerate}
\item Null plots should be generated by the same model
\item Only plots with the same aesthetics should be pooled for $\alpha$ estimation
\item Selection method should be the same (e.g. single or multiple target plots)
\end{enumerate}

When estimating $\hat\alpha$ from null plot selections in a standard lineup (e.g. a lineup containing at least one target plot), there is the possibility that no null plots are selected. A more reliable and systematic way to estimate $\hat\alpha$ would be to include a Rorshach lineup for each set of parameters used to generate null plots. These Rorshach lineups may be integrated into the testing procedure, or may be part of a pilot study used to assess the null plot generating model before it is used in lineups with data targets.

% \todo{default of ?? if no null plots are selected.} \svp{I know we talked about the default, I just don't remember what it was... as sum(log theta) -> - infinity, alpha -> 0 from what I can tell empirically. }

Intuitively, $\alpha$ is related to the proportion of null plots which would have some visually interesting feature: if $\alpha$ is low, that is, $\alpha << 1$, $\theta$s generated by the model would tend to be close to zero, with one or two larger panel selection probabilities (that is, one or two of the panels would be significantly more noticable). If $\alpha$ is high, $\alpha >> 1$, $\theta$s generated by the model would be closer to $1/m_{0}$, that is, each panel would be approximately equally likely to be selected. We know that the data generating model is likely to affect $\alpha$, and from \citet{vanderplas:2017} we know that the aesthetics can also significantly effect the selection of panels in a lineup.

In order to illustrate the variability in $\hat\alpha$ estimates across different lineup studies and to get a sense of the range of reasonable $\hat\alpha$ values, we estimated $\alpha$ for several previous single-target lineup studies of various sizes and designs. In several of these studies, multiple panel selections were allowed; these are allocated as partial selections to each set of counts.


<<null-alpha-single-target, fig.cap = "Alpha estimates for several lineup studies.", fig.width = 8, fig.height = 6, warning = F, message = F>>=
plot_df %>%
  ggplot(aes(x = dataset2, y = alpha, color = type, group = presentation)) +
  facet_wrap(~study, scales = "free_x", ncol = 4) +
  scale_color_manual("Panel Selection Type", values = c("orange4", "purple")) +
  geom_point(position = position_dodge(width = .3), shape = 1) +
  theme_bw() +
  theme(legend.position = "bottom") +
  theme(axis.title.x = element_blank()) +
  scale_y_continuous(expression(hat(alpha))) +
  ggtitle(expression(paste(hat(alpha), " for Single-Target Lineup Studies")), subtitle = "Estimated from 19 null plots")
@

\autoref{fig:null-alpha-single-target} shows the estimated $\alpha$s for each set of data, parameters, and aesthetics used in the studies. In most studies, $\hat\alpha \approx 0.07$ (5\% - 95\% quantiles: \Sexpr{sprintf("%0.3f and %0.3f", quantile(filter(plot_df, !study %in% c("Study 5", "Study 6"))$alpha, .05), quantile(filter(plot_df, !study %in% c("Study 5", "Study 6"))$alpha, .95))}), with very few estimated values over 0.10. There are some exceptions: Study 5 and Study 6 have $\hat\alpha$ values between \Sexpr{sprintf("%0.3f and %0.3f", min(filter(plot_df, study == "Study 5")$alpha), max(filter(plot_df, study == "Study 5")$alpha))}, which is a much wider range. In these studies, it also appears that plots which allowed participants to select multiple panels tend to have higher $\hat\alpha$ values, that is, multiple plot selections might allow for more diffusion of probabilities over multiple null plots. This effect might be heightened by increased lineup difficulty, which would also tend to increase $\hat\alpha$ values. One lineup from Study 5 is shown in \autoref{fig:hard-lineup}; only one participant identified the data target from among the nulls, suggesting that this is a difficult lineup, but the diffusion of identifications across many null panels indicates that the null data model generates plots with a relatively homogeneous level of visual interest.

\begin{figure}
\centering
\includegraphics[width=.6\textwidth]{lineup-images/turk11-filec2a72fe67dcf-multiple.pdf}
\caption{A lineup from Study 5. The target plot is in panel 7+7. Only one participant identified the target plot in 24 lineup evaluations. A total of 15 panels in this lineup were selected, indicating that the $\hat\alpha$ associated with this plot should be relatively large; in fact, $\hat\alpha =  $\Sexpr{round(alpha.ml(filter(studies_sum, study == "turk11" & str_detect(pic_name, "filec2a72fe67dcf-multiple"), obs_plot_location != response_no)$n), 4)}.}\label{fig:hard-lineup}
\end{figure}

The estimated $\hat\alpha$ value can then be used either in the specification of the null model $\alpha$ ($\alpha_{M_2}$) or in the calculation of the visual p-value under the frequentist framework. Currently, in either scenario, the data used in the calculation are the target plot identifications and the sum of all null plot identifications; the individual values of the null plot identifications are not used. These data can be reclaimed to estimate a general $\alpha$ for the overall null model generation scheme, or, in an ideal situation, Rorshach lineups could be used to estimate $\alpha$ directly without any possible contamination effects induced by the presence of target plots.

<<lineup-bf-data-setup, cache = F, include = F, warning = F>>=
# t10plot_data <- read_csv("data/Turk10/data/data-2-1-1-20-2-9-18.csv")
#
# counts_turk10 <- studies_sum %>%
#   ungroup() %>%
#   filter(study == "turk10" & pic_name == "filebab6ab9fb41-multiple.svg") %>%
#   group_by(study, response_no) %>%
#   summarize(n = sum(n)) %>%
#   mutate(bf = bf(1/2, 2, m = 20, c = n, k = sum(n))) %>%
#   mutate(.sample_inner = response_no)
#
#
# t13plot_data <- read_csv("data/Turk13/lineups/data/data-1-1-1-20-2-14-5.csv")
#
# counts_turk13 <- studies_sum %>%
#   ungroup() %>%
#   filter(study == "turk13" & pic_id == 142) %>%
#   group_by(study, response_no) %>%
#   summarize(n = sum(n))%>%
#   mutate(bf = bf(1/2, 2, m = 20, c = n, k = sum(n))) %>%
#   mutate(.sample_inner = response_no)
#
# counts_t6_sig <- studies_sum %>%
#   ungroup() %>%
#   filter(study == "turk6" & pic_id == 1047) %>%
#   group_by(study, response_no) %>%
#   summarize(n = sum(n))%>%
#   mutate(bf = bf(1/2, 2, m = 20, c = n, k = sum(n))) %>%
#   mutate(.sample = response_no)
#
# t6plot_data <- read_csv("data/Turk6/turk6_n100_96_1_4.csv")
# library(beeswarm)
#
# t6_sig_data <- t6plot_data %>%
# 		  split(.$.sample) %>%
# 		  purrr::map2_dfr(., (1:length(.)) == 1, function(x, y) {
# 		    if (y) {
# 		      dev.new(width = 8, height = 8)
# 		      beeswarm(formula = vals~group, data = x, add = F, do.plot = T, method = "swarm", cex = 2.5)
# 		    }
# 		    bs <- beeswarm(formula = vals~group, data = x, add = F, do.plot = F, method = "swarm", cex = 2.5)
# 		    data.frame(vals = bs$y.orig, group = bs$x.orig, newy = bs$y,
# 		               newx = bs$x, .sample = x$.sample[1])
# 		  })
# dev.off()
#
# t6_ambig_data <- read_csv("data/Turk6/turk6_60_96_1_8.csv")
# counts_t6_ambig <- studies_sum %>%
#   ungroup() %>%
#   filter(study == "turk6" & pic_id == 995) %>%
#   group_by(study, response_no) %>%
#   summarize(n = sum(n))%>%
#   mutate(bf = bf(1/2, 2, m = 20, c = n, k = sum(n))) %>%
#   mutate(.sample = response_no)
#
# save(counts_t6_ambig, t6_ambig_data, t10plot_data, counts_turk10, t13plot_data, counts_turk13, t6_sig_data, counts_t6_sig, file = "data/turk6_13_examples.Rdata")
load("data/turk6_13_examples.Rdata")
@

\section{Impact of $\hat\alpha$ Estimation}
We have already established that the choice of $\alpha$ has a large impact on the visual p-value (\autoref{fig:vis-p-val-sensitivity-initial}), but if we use the estimation method described in the last section, how do the results change in practice? Here, we show 3 different lineups - nonsignificant, marginal, and highly significant, and examine the visual p-values computed under the binomial distribution, by simulation using the \texttt{vinference} package (with default parameter $\alpha = 1$), and according to the beta-binomial model  in \autoref{eqn:beta-binomial} with $\alpha = 1$, and finally with $\alpha = \hat\alpha$ estimated from null plot selections in the experiment which originally included the lineup in question.

<<turk10-alpha-setup, include = F>>=
# source("code/compute_all_ps.R")

load("data/all_p_values_computed.Rdata")
all_p_val_methods <- all_p_val_methods %>%
  unnest_wider(p_calc)

lineup_info <- filter(all_p_val_methods, study == "turk10", str_detect(pic_name, "filebab6ab9fb41-multiple"))
lineup_res <- lineup_info[,c(8, 9, 11, 7, 12)] %>% as.numeric()
@

\autoref{fig:turk10-lineup} shows a plot which was evaluated a total of \Sexpr{sum(lineup_info$data[[1]]$n)} times, with \Sexpr{with(lineup_info$data[[1]], n[obs_plot_location == response_no])} target panel selections. The target panel was selected more frequently than any other panel, but a total of 10 panels were selected, and each selected panel was chosen by 10 or more participants. In this case, we would not expect the results to reveal a statistically significant effect, because many panels were selected almost as frequently as the target panel; in fact, the binomial p-value for this model is \Sexpr{sprintf("%0.4f, the simulated vinference p-value is %0.4f, the dirichlet-multinomial p-value with $\\alpha = 1$ is %0.4f, and the dirichlet-multinomial p-value with $\\hat\\alpha = %0.4f$ is %0.4f", lineup_res[1], lineup_res[2], lineup_res[3], lineup_res[4], lineup_res[5])}. This lineup represents a case that is not handled well by the binomial model or the multinomial-dirichlet model with noninformative $\alpha = 1$ selection, but is handled reasonably by the multinomial-dirichlet model with an informative choice for $\alpha$.


<<turk10-lineup, fig.cap = sprintf("A lineup from \\citet{loyAreYouNormal2015} which was evaluated a total of %d times. Number of selections $c_i$ are shown at the top left of each panel. The binomial p-value for this model is %0.4f, the simulated vinference p-value is %0.4f, the dirichlet-multinomial p-value with $\\alpha = 1$ is %0.4f, and the dirichlet-multinomial p-value with $\\hat\\alpha = %0.4f$ is %0.4f", sum(counts_turk10$n), lineup_res[1], lineup_res[2], lineup_res[3], lineup_res[4], lineup_res[5]), fig.width = 6, fig.height = 6/5*4.5, fig.align='center', out.width = ".66\\textwidth">>=
t10plot_data %>%
  ggplot() +
  # geom_ribbon(aes(x = naive1.env.fit.value, ymin = naive1.env.lower, ymax = naive1.env.upper), alpha = .1) +
  geom_point(aes(x = naive1.qq.x, y = naive1.qq.y)) +
  geom_abline(aes(slope = 1, intercept = 0), color = "grey30") +
  geom_text(aes(x = -Inf, y = Inf, label = sprintf("c[%d] == %.0f", .sample_inner, n)),
            hjust = -0.2, vjust = 1, parse = T, data = counts_turk10, inherit.aes = F) +
  facet_wrap(~.sample_inner) +
  theme(axis.text = element_blank(), axis.title = element_blank(), axis.ticks = element_blank())
@


Return to the 3 lineups shown in Bayes Factor section, provide estimated alphas and visual p-values for each plot.


\bibliographystyle{asa}
\bibliography{references}

\appendix
\section{Derivation of Visual p-value Distribution}
In the marginal case, where we have $K$ evaluations resulting in $C$ target plot evaluations of a $m$ plot lineup, we start with the following:
\begin{align*}
f(\theta | \alpha) & = \text{Beta}(\alpha, (m-1)\alpha)\\
& = \frac{\Gamma(m\alpha)}{\Gamma(\alpha)\Gamma((m-1)\alpha)} \cdot \theta^{\alpha - 1}(1-\theta)^{(m-1)\alpha - 1}\\
& = B\left(\alpha, (m-1)\alpha\right)\cdot \theta^{\alpha - 1}(1-\theta)^{(m-1)\alpha - 1}\\
\\
P(C |K, \theta) & = \text{Binomial}(C, K, \theta)\\
& = \binom{K}{C}\theta^C(1-\theta)^{K-C}
\end{align*}
By Bayes Theorem, if $A_1 = C + \alpha$ and $A_2 = K - C + (m-1)\alpha$,
\begin{align*}
f(\theta|C, K, \alpha) &= \frac{f(\theta|\alpha) P(C | \theta)}{P(C = c)}\\
&= \frac{B\left(\alpha, (m-1)\alpha\right)}{P(C=c)} \theta^{\alpha - 1}(1-\theta)^{(m-1)\alpha - 1}\cdot \binom{K}{C}\theta^C(1-\theta)^{K-C}\\
&= \frac{1}{P(C=c)} B\left(\alpha, (m-1)\alpha\right) \binom{K}{C} \theta^{A_1 - 1}(1-\theta)^{A_2 - 1}\\
\end{align*}

As $f(\theta|C, K, \alpha)$ is a probability distribution, it integrates to 1. So we can infer that
\begin{align*}
P(C=c) &= \int B\left(\alpha, (m-1)\alpha\right) \binom{K}{C} \theta^{A_1 - 1}(1-\theta)^{A_2 - 1} d\theta\\
& = B\left(\alpha, (m-1)\alpha\right) \binom{K}{C} \int \theta^{A_1 - 1}(1-\theta)^{X_2 - 1} d\theta\\
& = B\left(\alpha, (m-1)\alpha\right) \binom{K}{C} \frac{B(A_1, A_2)}{B(A_1, A_2)} \int \theta^{A_1 - 1}(1-\theta)^{A_2 - 1} d\theta\\
& =   \frac{B\left(\alpha, (m-1)\alpha\right) \binom{K}{C}}{B(A_1, A_2)\alpha)} \int B(A_1, A_2) \theta^{A_1 - 1}(1-\theta)^{A_2 - 1} d\theta\\
& =  \frac{B\left(\alpha, (m-1)\alpha\right) \binom{K}{C} }{B\left(C+\alpha, K-C+(m-1)\alpha\right)} \\
\end{align*}

Thus, the visual p-value for a lineup with $C$ target selections out of $K$ evaluations is
\begin{align}
P(x \geq C) & = \sum_{x=C}^K \frac{\binom{K}{x} B(\alpha, (m-1)\alpha) }{B(x+\alpha, K-x+(m-1)\alpha)}
\end{align}

A similar derivation holds in the full Dirichlet-Multinomial model.
\end{document}

