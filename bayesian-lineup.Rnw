\documentclass[12pt]{article}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{multirow}
\usepackage{hyperref}
% \usepackage{endfloat} % Figures to the end of the document

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
%---------------------------------------------------
%                 Editing Commands
\newcommand{\hh}[1]{{\color{magenta} #1}}
\newcommand{\svp}[1]{{\color{darkgray} #1}}
\newcommand{\fix}[1]{{\color{blue} #1}}
\newcommand{\todo}[1]{{\color{purple} #1}}

%---------------------------------------------------
%                 Placing Figures
\usepackage[font=small,skip=5pt]{caption}
\usepackage{subcaption}
\usepackage{afterpage}
\usepackage{rotating}
\usepackage[dvipsnames,svgnames,table]{xcolor}
\graphicspath{{figure/}}

%---------------------------------------------------
% Define new environment
\newtheorem{theorem}{Theorem}[section]
\newtheorem{algorithm}[theorem]{Algorithm}
%---------------------------------------------------

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%


\begin{document}

% \bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}{#1}\small\normalsize}
\spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind {
  \title{\bf A Bayesian approach to visual inference}
  \author{Susan VanderPlas, Heike Hofmann\thanks{
    The authors gratefully acknowledge funding from the National Science Foundation Grant \# DMS 1007697. All data collection has been conducted with approval from the Institutional Review Board IRB 10-347}\hspace{.2cm}\\
    Department of Statistics and Statistical Laboratory, Iowa State University}%
  \maketitle%
} \fi

\if1\blind {
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf A Bayesian approach to visual inference}
\end{center}
  \medskip
} \fi

\section{Introduction}

% Introduction to graphical testing: Rorschach vs. Visual Inference

% Graphical Testing vs. Statistical Testing - power improvements w/ additional people

% Something about the power of the human visual system? Don't want to get too off topic...
<<setup, echo = F, include = F>>=
library(tidyverse)
knitr::opts_chunk$set(echo = F, message = F)
# theme_set(theme_bw())
@

\section{A Bayesian's Statistical Lineup}
While visual inference was initially developed to mimic frequentist hypothesis tests with the standard threshold of $p = 0.05$, the method itself does not require adherence to frequentist methods. In this section, we show one possible Bayesian framework for visual inference, using a Dirichlet-multinomial distribution to represent the probabilities of selecting each subplot and the observed participant selections.

\subsection{Lineup Model Specification}
We will begin with a generic $m$-panel lineup, with selection probabilities $\theta_i, i = 1, ..., m$ where $\sum_{i=1}^m \theta_i = 1$, that is, the participant will select one (and only one) panel from the lineup as the most different. Our lineup has been evaluated by $K$ individuals, with $c_i, i = 1, ..., m$ the selection count for each panel, and $K = \sum_{i=1}^m c_i$.

A natural data model for this data is the Multinomial distribution, which has parameters $N, \bm{\theta}$, where $N$ describes the total number of events (that is, $N=K$) and $\bm{\theta} = \theta_1, ..., \theta_m$ describes the probabilities of each event occurring. We will fix $K$, as that is controlled by the experimental design, and model $\bm{\theta}$.

\begin{align}\label{eqn:multinomial-pmf}
f(\bm{c}|K, \bm{\theta}) & = \frac{K!}{c_1! \cdots c_m!} \prod_{i=1}^m \theta_i^{c_i}
\end{align}

We assign prior probability to $p$ using a Dirichlet distribution with concentration hyperparameter $\bm{\alpha}$, which happens to be conjugate to the multinomial distribution. As the position of the panels within the lineup are random, we use a symmetric Dirichlet distribution, with $\alpha_i = \alpha, i = 1, ..., m$, that is, the concentration hyperparameter is constant. This allows us to vary the lineup difficulty through the hyperparameter $\alpha$ without having to specify which plot $i$ is the target plot.

The pdf of the symmetric Dirichlet distribution is
\begin{align}\label{eqn:dirichlet-pdf}
f(\bm{\theta}|\alpha) & = \frac{\left(\Gamma(\alpha)\right)^m}{\Gamma(m\alpha)} \prod_{i=1}^m \theta_i^{\alpha - 1}
\end{align}

Using the conjugate relationship between the Dirichlet and Multinomial distributions, we then get the posterior distribution as the Dirichlet$(\bm{c + \alpha})$ distribution.

\begin{align}\begin{split}
(\alpha_1, ..., \alpha_m) = \bm{\alpha} &= \text{concentration hyperparameter}\\
(c_1, ..., c_m) = \bm{c} &= \text{observed plot selections}, \sum_{i=1}^m c_i = K\\
p(\bm{\theta}) &\sim Dirichlet(\bm\alpha) \\
f(\bm{c} | \bm{\theta}) & \sim Multinomial(\bm\theta, K)\\
p(\bm\theta | \bm{c}, \bm{\alpha}) & \sim Dirichlet(\bm{c} + \bm{\alpha})
\end{split}\end{align}

where $Multinomial(\bm\theta, K)$ is defined as in \autoref{eqn:multinomial-pmf} and $Dirichlet(\bm\alpha)$ is defined as in \autoref{eqn:dirichlet-pdf}.

Typically, when evaluating lineups, we compare the number of target plot identifications with the aggregate number of null plot identifications (see \citet{majumder2013validation}). This is equivalent to the marginal distribution of $c_t$, where $t \in 1, ..., m$ is the index of the target panel in the lineup. The equivalent Bayesian version takes the form of a Beta-Binomial model. The frequentist model assumes $\theta_t = 1/m$, that is, $\theta_t$ is fixed and equal to the selection probability of every other panel in the lineup.

\begin{align}\begin{split}
\alpha &= \text{concentration hyperparameter}\\
c_t &= \text{target plot selections},\\
K &= \text{total evaluations}\\
p(\theta_t) &\sim Beta(\alpha, (m-1)\alpha) \\
f(c_t | \theta_t) & \sim Binomial(\theta_t, K)\\
p(\theta_t | \bm{c_t}, \alpha) & \sim Beta(c_t + \alpha, K - c_t + (m-1)\alpha)
\end{split}\end{align}

We know that it is unreasonable to assume that the selection probability of every null plot is equal: null plots are randomly generated, and occasionally, the randomly generated plot will have an interesting feature (that may or may not be present in the target plot). When that occurs, the interesting null plot will be selected more frequently than the other nulls, despite being generated by the same distribution. The human perceptual process is very efficient at identifying differences - it was important to be able to spot a predator amongst the leaves, and as a result, we will gravitate towards a plot which is different on some measure, if not the measure that's under investigation. As a result, it's important to understand the effect of $\alpha$ on the posterior distribution.


\subsection{Dirichlet Hyperparameter}
% Stolen from heike/lineup-scenarios
When $\alpha = 1$ the symmetric Dirichlet distribution is uniform on the $m-1$ dimensional simplex. When $\alpha < 1$ the mass of the distribution is along the edges of the simplex, where most values of $\theta_i$ will be close to 0. When $\alpha>1$, the mass of the distribution is in the center of the simplex, with most of the $\theta_i$ having similar values. \autoref{fig:simplex} shows ternary plots of values simulated from a 3-dimensional Dirichlet distribution which illustrate this effect.

<<simplex, echo=FALSE, out.width='\\textwidth', fig.width=12, fig.height=4, fig.cap = "Dirichlet distributed samples on the 2-dimensional simplex.">>=
simplex <- purrr::map_df(.x = c(1/3, 1, 3), ~data.frame(
  gtools::rdirichlet(5000, alpha = rep(.x, 3))
), .id = "alpha") %>%
  rename(p1 = X1, p2 = X2, p3 = X3) %>%
  mutate(alphalabel = factor(alpha, levels = 1:3, labels = c("alpha: 1/3", "alpha: 1", "alpha: 3")))

ggtern::ggtern(aes(x = p1, y = p2, z = p3), data = simplex) +
  geom_point(alpha = .075, shape = 16) +
  facet_wrap(~alphalabel, labeller = "label_parsed") +
  theme(panel.background = element_rect(colour = "black"))
@
% End stolen from heike/lineup-scenarios

In conjunction with the Multinomial data model and a symmetric Dirichlet prior, $\alpha$ provides the equivalent of pseudo-observations for each plot; that is, the effect of $\alpha$ is equivalent to adding $\alpha$ observations to each panel in the lineup. When $\alpha$ is small, these pseudo-observations have relatively little influence, but when $\alpha$ is large, the pseudo-observations can quickly dwarf any information provided by the data. This is particularly true for the marginal Beta-Binomial model, where the equivalent of $(m-1)\alpha$ pseudo-observations are added. In most lineup studies, a plot might be evaluated between 10 and 30 times; with a $m=20$ lineup, $\alpha = 1$ can easily dominate the data.

\subsection{Bayes Factors vs. P-values}
In the standard analysis of a lineup plot, with $C$ data detections in $K$ independent evaluations, a visual p-value is calculated using the mass function $$P(C\geq x) = \sum_{x = C}^{K} \binom{K}{x} \frac{1}{B(\alpha, (m-1)\alpha)}\cdot B(x+\alpha, K-x+(m-1)\alpha)$$
This value has been computed in the past by simulation (assuming $\alpha=1$), but the more general solution is useful to consider in this context. The visual p-value is then compared to a threshold of 0.05, and if it is smaller, we reject the hypothesis that every plot is equally likely to be selected.

Bayes factors are somewhat analogous to the frequentist hypothesis test, but to construct a bayes factor for a lineup, we would want to consider the marginal distribution consisting of the number of data detections and the number of null detections, as the primary factor in lineup evaluation is whether the data plot can be detected from among the collective set of null plots. The marginal beta-binomial model considers the data detections and the aggregated null-plot detections; as a result, the parameters of the posterior beta distribution are $C + \alpha, K - C + (m-1)\alpha$.

In order to construct a Bayes Factor which would be roughly equivalent to the standard lineup evaluation, we would compare two models with different hyperparameters: the model structure is the same, but in the first model, we would expect one or two of the panels to be selected more frequently; in the second, we would expect all panels to be selected approximately equally.

\begin{description}
\item[M1] $\alpha < 1$ - a small value of $\alpha$ would indicate that one or two of the plots might tend to be selected more frequently.
\item [M2] $\alpha \geq 1$ - a large value of $\alpha$ would suggest that each plot is (approximately) equally likely to be selected.
\end{description}

% Derive bayes factors for plots based on one-target lineups
Let $\alpha_{M_1} < 1$ be the prior concentration hyperparameter for model 1, and $\alpha_{M_2} \geq 1$ be the prior concentration hyperparamer for model 2. If we assume that both models are equally likely a priori, then the prior model odds cancel, and we can derive the bayes factor comparing model 1 to model 2 for panel $i$ in a lineup as:
\begin{align}
BF(M_1, M_2)_i  =& \frac{\displaystyle\int_\theta p(\theta | \alpha_{M_1}) f(K, c_i | \theta, \alpha_{M_1}) d\theta}
                        {\displaystyle\int_\theta p(\theta | \alpha_{M_2}) f(K, c_i | \theta, \alpha_{M_2}) d\theta} \nonumber\\
= &\frac{\displaystyle\int_\theta \binom{K}{c_i} \theta^{c_i} (1 - \theta)^{K - c_i}
         \frac{1}{B(\alpha_{M_1}, (m-1) \alpha_{M_1})} \theta^{\alpha_{M_1}} (1-\theta)^{(m-1) \alpha_{M_1}} d\theta}
        {\displaystyle\int_\theta \binom{K}{c_i} \theta^{c_i} (1 - \theta)^{K - c_i}
         \frac{1}{B(\alpha_{M_2}, (m-1) \alpha_{M_2})} \theta^{\alpha_{M_2}} (1-\theta)^{(m-1) \alpha_{M_2}} d\theta}\nonumber\\
= &\frac{B(\alpha_{M_2}, (m-1)\alpha_{M_2})}{B(\alpha_{M_1}, (m-1)\alpha_{M_1})}
   \frac{B(c_i + \alpha_{M_1}, K - c_i + (m-1)\alpha_{M_1})}{B(c_i + \alpha_{M_2}, K - c_i + (m-1)\alpha_{M_2})} \times \nonumber\\
  &\frac{\displaystyle\int_\theta \frac{1}{B(c_i + \alpha_{M_1}, K - c_i + (m-1)\alpha_{M_1})} \theta^{c_i + \alpha_{M_1} - 1} (1-\theta)^{K - c_i + (m-1)\alpha_{M_1} - 1} d\theta}
        {\displaystyle\int_\theta \frac{1}{B(c_i + \alpha_{M_2}, K - c_i + (m-1)\alpha_{M_2})} \theta^{c_i + \alpha_{M_2} - 1} (1-\theta)^{K - c_i + (m-1)\alpha_{M_2} - 1} d\theta}\nonumber\\
 = & \frac{B(\alpha_{M_2}, (m-1)\alpha_{M_2})}{B(\alpha_{M_1}, (m-1)\alpha_{M_1})}
\frac{B(c_i + \alpha_{M_1}, K - c_i + (m-1)\alpha_{M_1})}{B(c_i + \alpha_{M_2}, K - c_i + (m-1)\alpha_{M_2})}
\end{align}

It would also be possible to calculate a Bayes factor for the entire multidimensional vector of panel selections, but this is less useful for determining whether the target plot is noticably more likely to be selected than the null plots, as it tests all of the panels as a single unit, with no differentiation based on null or target data. Rather, the multidimensional Bayes Factor for a lineup tests whether one or more panels are more likely to be selected (compared to a model where all panels are equally likely to be selected); it does not indicate which panel(s) are more likely.

\begin{align}
BF(M_1, M_2) = &  \frac{\displaystyle\int_\theta p(\theta | \alpha_{M_1}) f(\bm{c} | \bm\theta, \alpha_{M_1}) d\bm\theta}
                         {\displaystyle\int_\theta p(\theta | \alpha_{M_2}) f(\bm{c} | \bm\theta, \alpha_{M_2}) d\bm\theta} \nonumber\\
              =  &  \frac
              {\int\left(\frac{1}{B(\bm\alpha_1)} \prod_{i=1}^m \theta_i^{\alpha_1 - 1}\right)\left(\frac{\left(\sum_{i=1}^m c_i\right)!}{c_1!\times \cdots\times c_m!} \prod_{i=1}^m \theta_i^{x_i}\right) d\bm\theta}
              {\int\left(\frac{1}{B(\bm\alpha_2)} \prod_{i=1}^m \theta_i^{\alpha_2 - 1}\right)\left(\frac{\left(\sum_{i=1}^m c_i\right)!}{c_1!\times \cdots\times c_m!} \prod_{i=1}^m \theta_i^{x_i}\right) d\bm\theta}\\
              =  & \frac{B(\bm\alpha_2)}{B(\bm\alpha_1)}
              \frac{\frac{\left(\sum_{i=1}^m c_i\right)!}{c_1!\times \cdots\times c_m!}}{\frac{\left(\sum_{i=1}^m c_i\right)!}{c_1!\times \cdots\times c_m!}}
              \frac{\int \prod_{i=1}^m \theta_i^{\alpha_1 - 1 + c_i}d\bm\theta}{\int \prod_{i=1}^m \theta_i^{\alpha_2 - 1 + c_i}d\bm\theta}\\
              = & \frac{B(\bm\alpha_2)B(\bm\alpha_1+\bm{c})}{B(\bm\alpha_1)B(\bm\alpha_2+\bm{c})}
\end{align}

<<bayes-factors-alphas, fig.width = 8, fig.height = 4, fig.cap = "Bayes factor values at different levels of $\\alpha_1$ and $\\alpha_2$. If there are large numbers of data panel identifications, the Bayes Factor will be large even with conservative parameter values (e.g. $\\alpha_1$ near 1, $\\alpha_2 =  2$). The choice of alphas is much more impactful when there are fewer data plot identifications. While the criteria for interpretation of Bayes factors vary, a result of $>20$ is generally considered to correspond to strong evidence toward $M_1$ over $M_2$." >>=
mvbeta <- function(alpha, log = F) {
  z <- sum(lgamma(alpha)) - lgamma(sum(alpha))
  if (!log) return(exp(z)) else return(z)
}

bf <- function(a1, a2, m = 20, c, k = sum(c)) {
  stopifnot(a1 > 0, a2 > 0, c <= k, m > 1)

  beta(a2, (m - 1)*a2) * beta(c + a1, k - c + (m - 1)*a1) /
    (beta(a1, (m - 1)*a1) * beta(c + a2, k - c + (m - 1)*a2))
}
bf_vec <- function(a1, a2, m = 20, c, k = sum(c)) {
  stopifnot(a1 > 0, a2 > 0, c <= k, m > 1)

 exp(mvbeta(rep(a2, length(c)), log = T) + mvbeta(a1 + c, log = T) -
   mvbeta(rep(a1, length(c)), log = T) - mvbeta(a2 + c, log = T))
}


breaks <-  c(2, 5, 10, 20)
data_breaks <- c(1, 3, 5, 8, 10)


p01 <- tidyr::crossing(a1 = seq(.01, 1, .01),
                a2 = breaks,
                m = 20,
                k = 20,
                c = data_breaks) %>%
  mutate(bf = purrr::pmap_dbl(., bf)) %>%
  mutate(a2f = factor(a2, levels = unique(a2), labels = sprintf("alpha[2] == %f", unique(a2)), ordered = T)) %>%
  ggplot(aes(x = a1, y = bf, color = factor(c), group = factor(c))) +
  geom_line() +
  scale_y_log10("Bayes Factor", breaks = c(1, 10, 100, 1000, 10000)) +
  scale_x_continuous(expression(alpha[1])) +
  scale_color_discrete("# Data\nPanel\nIdentifications\n(K = 20)", guide = F) +
  facet_wrap(~a2f, labeller = "label_parsed")

p02 <- tidyr::crossing(a2 = seq(1.5, 20, by = 0.1),
                a1 = 1/breaks,
                m = 20,
                k = 20,
                c = data_breaks) %>%
  mutate(bf = purrr::pmap_dbl(., bf)) %>%
  mutate(a2f = factor(a2, levels = unique(a2),
                      labels = sprintf("alpha[2] == ~~%f", unique(a2)),
                      ordered = T)) %>%
  mutate(a1f = factor(a1, levels = 1/rev(breaks),
                     labels = sprintf("alpha[1] == ~~1/%f", rev(breaks)),
                     ordered = T)) %>%
  ggplot(aes(x = a2, y = bf, color = factor(c), group = factor(c))) +
  geom_line() +
  scale_y_log10("Bayes Factor", breaks = c(1, 10, 100, 1000, 10000)) +
  scale_x_continuous(expression(alpha[2])) +
  scale_color_discrete("# Data\nPanel\nIdentifications\n(K = 20)") +
  facet_wrap(~a1f, labeller = "label_parsed")

gridExtra::grid.arrange(p01, p02, nrow = 1, widths = c(.425, .575))
@
\autoref{fig:bayes-factors-alphas} shows Bayes Factors in the marginal case for combinations of $\alpha_1, \alpha_2$ over several different $C$ values, assuming a total of $K=20$ independent evaluations.

Examining the performance of this Bayes factor with a lineup from \citet{loyAreYouNormal2015}, shown in \autoref{fig:turk13-lineup}, we can see that there is no significant support for the choice of model 1 over model 2, or vice versa, when considering the target plot compared to all null plot selections. The target plot, which is in panel 14, had relatively few selections, resulting in a marginal Bayes Factor of 1.03. The multidimensional Bayes Factor here indicates that there is some evidence that all plots are not equally likely to be selected.

<<turk13-data-setup, cache = F, include = F>>=
library(qqplotr)
turk13 <- read_csv("data/Turk13/turk13.csv",
                   col_types = "i_c__c___dcccic_ic_") %>%
  mutate(indiv_id = factor(nick_name) %>% as.numeric()) %>%
  select(pic_id, indiv_id, response_no, sample_size, test_param, param_value, obs_plot_location, pic_name, difficulty, data_name)

t13_pic_details <- read_csv("data/Turk13/lineups/picture-details.csv")

t13plot_data <- read_csv("data/Turk13/lineups/data/data-1-1-1-20-2-14-5.csv")

counts <- filter(turk13, pic_id <= 2) %>%
  count(response_no) %>%
  mutate(response_no = as.numeric(response_no)) %>%
  complete(response_no = 1:20, fill = list(n = 0)) %>%
  mutate(bf = bf(1/2, 2, m = 20, c = n, k = sum(n))) %>%
  mutate(.sample_inner = response_no)

@

<<turk13-lineup, fig.cap = sprintf("A lineup from \\citet{loyAreYouNormal2015} which was evaluated a total of %d times. Number of selections $c_i$ are shown at the top left of each panel; the calculated Bayes Factors with $\\alpha_1 = 0.5$ and $\\alpha_2 = 2$ is shown at the bottom right of the target panel (panel 14). As panel 4 was selected more frequently than panel 14, there is no significant information in favor of model 1 (the target plot is more likely to be selected than the nulls) over model 2. The multidimensional (whole plot) bayes factor with the same $\\alpha_1,\\alpha_2$ values is %.2f, indicating that there is weak to moderate evidence that all plots are not equally likely to be selected; that is, that model 1 better aligns with the observed data than model 1. Note that this evidence support is primarily due to the multiple selections of panel 4, which shows data generated by the null data model.", sum(counts$n), bf_vec(1/2, 2, c = counts$n))>>=
t13plot_data %>%
  ggplot() +
  geom_ribbon(aes(x = naive1.env.fit.value, ymin = naive1.env.lower, ymax = naive1.env.upper), alpha = .1) +
  geom_point(aes(x = naive1.qq.x, y = naive1.qq.y)) +
  geom_abline(aes(slope = 1, intercept = 0), color = "grey30") +
  geom_text(aes(x = -Inf, y = Inf, label = sprintf("c[%d] == %.0f", .sample_inner, n)),
            hjust = -0.2, vjust = 1, parse = T, data = counts, inherit.aes = F) +
  geom_text(aes(x = Inf, y = -Inf, label = sprintf("BF == %.2f", bf)),
            vjust = -0.2, hjust = 1, parse = T, data = filter(counts, .sample_inner == 14), inherit.aes = F) +
  facet_wrap(~.sample_inner) +
  theme(axis.text = element_blank(), axis.title = element_blank())
@

The primary advantage of the Bayes factor method for single-target lineup analysis is that it seamlessly incorporates multiple target plot selections: a participant who selects $p$ panels of a lineup would contribute $1/p$ votes to each panel's total. Bayes factors become more useful, however, when used to examine a two-target lineup, where two models are competing against each other, with null plots drawn from a mixture of the two distributions. In this case, for target panels $i, j$ in a $m$-panel lineup, we can calculate three single-panel bayes factors based on models $M_A$, that the first data model target is more likely to be selected, $M_B$, that the second data model target is more likely to be selected ($M_A$ and $M_B$ are separate instances of $M_1$ in the single target case, representing different target panels), and $M_U$, a generalization of $M_2$ in the single target case, which suggests that all plots are equally likely to be selected.
\begin{align}
BF(M_A, M_U)_i &=  \frac{B(\alpha_{M_U}, (m-2)\alpha_{M_U})}{B(\alpha_{M_A}, (m-2)\alpha_{M_1})}
\frac{B(c_i + \alpha_{M_A}, K - c_i - c_j + (m-2)\alpha_{M_A})}{B(c_i + \alpha_{M_U}, K - c_i - c_j + (m-2)\alpha_{M_U})}\\
BF(M_B, M_U)_j &=  \frac{B(\alpha_{M_U}, (m-2)\alpha_{M_B})}{B(\alpha_{M_2}, (m-2)\alpha_{M_2})}
\frac{B(c_j + \alpha_{M_B}, K - c_i - c_j + (m-2)\alpha_{M_2})}{B(c_j + \alpha_{M_U}, K - c_i - c_j + (m-2)\alpha_{M_U})}\\
\end{align}

The first two, $BF(M_1, M_2)_i$ and  $BF(M_1, M_2)_j$, examine the relative likelihood of model 1 vs. model 2. \todo{The third bayes factor compares the probability of selection of plot $i$ with the probability of selection of plot $j$ to establish which data generation model is more visually salient.}

\todo{Show Bayes Factor for one single-target and one two-target lineup}

\section{Alpha and Null Plots}
\todo{Fix this introduction to estimating alpha - it's relevant to the frequentist calcs too}
One issue with lineup experiments is that participants sometimes identify features in the null plots which are interesting, but not necessarily on the same dimension as the interesting feature in the data plot. In some cases, this is because the lineup is particularly challenging, but in others, it can be an indicator that the null plot generation model is not adequately reproducing features in the data.  - \todo{cite Adam's normality diagnostic paper} In large lineup experiments, it can be difficult to screen every plot for these issues, resulting in systematic problems such as those in \citet{vanderplas:2017}, where clustering algorithms applied to null plot data occasionally resulted in clusters of 1-3 points.

One possible solution is to run a small pilot study in which a Rorshach lineup is used to examine the selection frequency of different null plots. If one or two null plots are systematically selected from this Rorshach lineup, then the data-generating model may need to be examined. \svp{In addition, it may be possible to estimate $\alpha_2$ from the null plots alone and compare to $\alpha_1$ estimated from all of the plots. XXX this is nuts, right? I'm not even sure it would be valid, but I think crazier things have been done in the name of Empirical Bayes...}

% modified from heike/lineup-scenarios
In order to estimate $\alpha$ from the null plots of a lineup, let $m_0$ be the number of null plots in an $m$- panel lineup. If the lineup is a Rorshach lineup, then $m=m_0$. We can estimate $\alpha$ using a set of such lineups, $j=1, ..., n$. The likelihood function is then
\begin{align}
\mathscr{L}(\alpha|\theta) &= \prod_{j=1}^n \left(\frac{1}{B(\alpha)}\right)^{m_0} \prod_{i=1}^{m_0} \theta_{ij}^{\alpha - 1}\nonumber\\
& = \left(\frac{\Gamma(\alpha m_0)}{\left(\Gamma(\alpha)\right)^{m_0}}\right)^n \prod_{ij} \theta^{\alpha-1}_{ij}
\end{align}
and the derivative of the log-likelihood function can be calculated as
\begin{align}
\ln \mathscr{L}(\alpha|\theta) & = n \ln \Gamma(\alpha m_0) - nm_0\ln\Gamma(\alpha) + \sum_{ij}(\alpha-1)\ln \theta_{ij}\nonumber\\
\frac{d}{d\alpha}\ln \mathscr{L}(\alpha|\theta) &= nm_0\psi(\alpha m_0) - nm_0\psi(\alpha) + \sum_{ij} \ln \theta_{ij}
\end{align}
where $\psi(x)$ is the digamma function, $\psi(x) = \frac{d}{dx}\ln\Gamma(x)$.
Setting this to zero, we find that the MLE of $\alpha$ can be obtained empirically from the sum of the log probabilities $\theta_{ij}$.
\begin{align}
0 &= nm_0\psi(\alpha m_0) - nm_0\psi(\alpha) + \sum_{ij} \ln \theta_{ij} \nonumber\\
nm_0\psi(\alpha) - nm_0\psi(\alpha m_0) &= \sum_{ij} \ln \theta_{ij}\nonumber\\
\psi(\alpha) - \psi(\alpha m_0) & = \frac{1}{nm_0}\sum_{ij} \ln \theta_{ij} \label{eqn:mle}
\end{align}
% end modified from heike/lineup scenarios

The second derivative of the log likelihood function uses the trigamma function, $\psi_1(x) = \frac{d^2}{dx^2} \ln \Gamma(x)$. $\psi_1(x)$ can also be written as the series $\psi_1(x) = \sum_{z=0}^\infty \frac{1}{(z + n)^2}$. Thus,
\begin{align}
\frac{d^2}{d\alpha^2}\ln \mathscr{L}(\alpha|\theta) &= nm_0^2\psi_1(\alpha m_0) - nm_0\psi_1(\alpha)\\
& = nm_0^2\left(\sum_{x=0}^\infty \frac{1}{(\alpha m_0 + x)^2} - \sum_{x=0}^\infty \frac{1}{m_0(\alpha + x)^2}\right)\nonumber\\
& = nm_0^2\sum_{x=0}^\infty \left(\frac{1}{(\alpha m_0 + x)^2} - \frac{1}{m_0(\alpha + x)^2}\right) \leq 0 \text{ for } m \geq 1, \alpha > 0 \nonumber
\end{align}

Thus, the MLE of $\alpha$ is the $\alpha$ satisfying the equation \autoref{eqn:mle}. This formulation allows for the combination of evaluations of Rorshach lineups and lineups with one or more target plots, but in order for this combination of data to be meaningful, the following conditions should hold:
\begin{enumerate}
\item Null plots should be generated by the same model
\item Only plots with the same aesthetics should be pooled for $\alpha$ estimation
\end{enumerate}

When estimating $\hat\alpha$ from null plot selections in a standard lineup (e.g. a lineup containing at least one target plot), there is the possibility that no null plots are selected.

\todo{default of ?? if no null plots are selected.} \svp{I know we talked about the default, I just don't remember what it was... as sum(log theta) -> - infinity, alpha -> 0 from what I can tell empirically. }

Intuitively, $\alpha$ describes the proportion of null plots which would have some visually interesting feature: if $\alpha$ is low, that is, $\alpha << 1$, $\theta$ generated by the model would tend to be zero, with one or two nonzero probabilities (that is, one or two of the plots would be significantly more noticable). If $\alpha$ is high, $\alpha >> 1$, $\theta$s generated by the model would be closer to $1/m_{0}$, that is, each plot would be approximately equally likely to be selected. We know that the data generating model would affect $\alpha$, and from \citet{vanderplas:2017} we know that the aesthetics significantly effect the selection of plots in a lineup.

% stolen from heike/lineup-scenarios
<<alpha-ml, include = F>>=
#' mle of alpha
alpha.ml <- function(p, weight = NULL, eps = 10^(-7)) {
  # matrix p
  if (is.null(dim(p))) {
    m <- length(p)
    n <- 1
    p <- matrix(p, nrow = 1)
  } else {
    n <- dim(p)[1]
    m <- dim(p)[2]
  }
  if (is.null(weight)) {
    weight <- rep(1, n)
  } else {
    n <- sum(weight)
  }

  weight <- weight / sum(weight)
  ps <- p
  ps[p < eps] <- eps # make sure we don't take the log of 0
  ps <- ps / rowSums(ps)
  logp <- sum(weight * rowSums(log(ps))) / m
  ml <- function(alpha) {
    digamma(alpha) - digamma(alpha * m) - logp
  }
  # find alpha such that digamma(alpha) - digamma(alpha*m) = logp
  alpha <- uniroot(f = ml, interval = c(eps, 100))

  alpha$root
}

@
% end stolen from heike/lineups scenarios
<<null-alpha-est>>=
aes_order <- c("colorShapeEllipse", "colorTrend", "colorEllipse", "colorShape", "shape", "color", "trend", "plain", "colorEllipseTrendError", "trendError")
pic_data <- read_csv("data/Turk16/data-picture-details-gini.csv")
t16_res <- read_csv("data/Turk16/turk16_results_anon.csv", col_types = "_c__cc___") %>%
  mutate(pic_id = str_trim(pic_id) %>% as.numeric()) %>%
  filter(!is.na(pic_id)) %>%
  mutate(response_no = purrr::map(response_no, str_split, pattern = ",", simplify = F) %>%
           purrr::map(unlist),
         response_no = purrr::map(response_no, as.numeric),
         n_results = purrr::map_int(response_no, length),
         weight = 1/n_results) %>%
  select(-matches("ip_address|time|conf_level|choice_reason")) %>%
  unnest(response_no) %>%
  group_by(pic_id, response_no) %>%
  summarize(weight = sum(weight)) %>%
  complete(pic_id, response_no = 1:20, fill = list(weight = 0)) %>%
  left_join(select(pic_data, test_param, obs_plot_location, pic_name, data_name) %>%
              mutate(pic_id = 1:n())) %>%
  filter(!is.na(response_no)) %>%
  extract(obs_plot_location, into = c("cluster_target", "trend_target", "gini_target"),
          regex = c("(\\d{1,}), ?(\\d{1,}), ?(\\d{1,})")) %>%
  mutate_at(vars(cluster_target:gini_target), as.numeric) %>%
  mutate(is_target = ((response_no == cluster_target) | (response_no == trend_target)),
         null_no = factor(response_no*(!is_target)) %>% as.numeric %>% magrittr::subtract(1)) %>%
  group_by(pic_id) %>%
  mutate(null_n = sum((!is_target)*weight)) %>%
  mutate(
    set_number = str_extract(data_name, "set-(\\d{1,})") %>% str_remove("set-") %>% as.numeric,
    set_type = str_remove(data_name, "set-\\d{1,}-"),
    aes_type = str_remove(test_param, "turk16-"))

t16_null_wide <- t16_res %>%
  select(pic_id, set_number, set_type, aes_type, null_no, weight) %>%
  group_by(pic_id, set_type, aes_type) %>%
  filter(null_no > 0) %>%
  mutate(n_total = sum(weight)) %>%
  filter(n_total > 0) %>%
  tidyr::spread(key = null_no, value = weight) %>%
  ungroup()


t16_null_alpha_plotwise <- t16_null_wide %>%
  extract(set_type, into = c("k", "sdline", "sdgroup"),
          regex = "k-([35])-sdline-([\\d\\.]{1,})-sdgroup-([\\d\\.]{1,})") %>%
  mutate_at(vars(k, sdline, sdgroup), as.numeric) %>%
  group_by(aes_type, set_number, k, sdline, sdgroup, pic_id, n_total) %>%
  nest(`1`:`18`) %>%
  mutate(alpha = purrr::map2_dbl(data, n_total,
                                 function(x, y) alpha.ml(as.matrix(x), weight = y))) %>%
  mutate(aes_type = factor(aes_type, levels = aes_order, ordered = T)) %>%
  mutate(alpha_type = "null") %>%
  group_by(k, sdline, sdgroup) %>%
  mutate(set_min = min(set_number))

t16_null_alpha_setwise <- t16_null_wide %>%
  extract(set_type, into = c("k", "sdline", "sdgroup"),
          regex = "k-([35])-sdline-([\\d\\.]{1,})-sdgroup-([\\d\\.]{1,})") %>%
  mutate_at(vars(k, sdline, sdgroup), as.numeric) %>%
  group_by(aes_type, k, sdline, sdgroup) %>%
  nest() %>%
  mutate(n = purrr::map(data, "n_total")) %>%
  mutate(alpha = purrr::map2_dbl(data, n, function(x, y) alpha.ml(as.matrix(x), weight = y))) %>%
  mutate(aes_type = factor(aes_type, levels = aes_order, ordered = T)) %>%
  mutate(alpha_type = "null") %>%
  mutate(n_total = purrr::map_dbl(n, sum)) %>%
  group_by(k, sdline, sdgroup) %>%
  mutate(set_min = min(purrr::map_dbl(data, ~min(.$set_number))),
         set_max = max(purrr::map_dbl(data, ~max(.$set_number))))


t16_null_alpha_aeswise <- t16_null_wide %>%
  extract(set_type, into = c("k", "sdline", "sdgroup"),
          regex = "k-([35])-sdline-([\\d\\.]{1,})-sdgroup-([\\d\\.]{1,})") %>%
  mutate_at(vars(k, sdline, sdgroup), as.numeric) %>%
  group_by(aes_type) %>%
  nest() %>%
  mutate(n = purrr::map(data, "n_total")) %>%
  mutate(alpha = purrr::map2_dbl(data, n, function(x, y) alpha.ml(as.matrix(x), weight = y))) %>%
  mutate(aes_type = factor(aes_type, levels = aes_order, ordered = T)) %>%
  mutate(alpha_type = "null") %>%
  mutate(n_total = purrr::map_dbl(n, sum))

t16_all_wide <- t16_res %>%
  select(pic_id, set_type, aes_type, response_no, weight) %>%
  group_by(pic_id, set_type, aes_type) %>%
  # filter(null_no > 0) %>%
  mutate(n_total = sum(weight)) %>%
  filter(n_total > 0) %>%
  tidyr::spread(key = response_no, value = weight) %>%
  ungroup()

@


\section{Evaluating Null Plot Generation Models}
The null plot generation model used in \citet{vanderplas:2017} had a noticeable deficiency: the aesthetics used in the plots required clustering of the null plot data after data generation, and the cluster size was not well-controlled. As a result, some of the generated null plots contained ``clusters" consisting of only of one or two points, while other artificially imposed clusters were noticably diffuse or oddly shaped. In the case of clusters of one or two points, estimation of a 95\% bounding ellipse failed, providing an unintended feature which participants noticed, resulting in significant null plot identifications (\autoref{fig:featurehierarchy-null-model-sucks} shows two such images).

<<featurehierarchy-null-model-sucks, fig.width = 12, fig.height = 6, fig.cap = "A two-target lineup from \\citet{vanderplas:2017}, plotted with different aesthetic combinations. The two targets are in plot 11+8 (clustering) and 2+2 (linear trend). In plots 10, 11, 15, and  17, the estimation of a bounding ellipse in a null plot failed due to insufficient points in the cluster. This deficiency is less apparent in the second lineup, which only has a regression line, as the aesthetics which emphasize grouping are not shown.", warning = F, message=F>>=
plot_names <- c("plain","color", "shape", "colorShape", "colorEllipse", "colorShapeEllipse", "trend", "trendError", "colorTrend", "colorEllipseTrendError")
fix_plot_names <- function(x) {
  x %>%
  str_replace("color", "Color + ") %>%
  str_replace("[sS]hape", "Shape + ") %>%
  str_replace("[tT]rend", "Trend + ") %>%
  str_replace("Ellipse", "Ellipse + ") %>%
  str_replace("Error", "Error + ") %>%
  str_replace("plain", "Plain") %>%
  str_replace("( \\+ )$", "") %>%
  str_replace_all(c("Color" = "C", "Shape" = "S", "Ellipse" = "L", "Trend" = "T", "Error" = "E")) %>%
  str_replace_all(" \\+ ", "+") %>%
    factor(levels = c("T", "T+E", "Plain", "S", "C", "C+S", "C+L", "C+S+L", "C+T", "C+L+T+E"), ordered = T)
}

source("code/GenerateLineups.R")
lineup_dat <- read_csv("data/Turk16/Image_Data/set-46-k-5-sdline-0.45-sdgroup-0.25.csv")

lineup_alphas <- filter(t16_null_alpha_plotwise, set_number == 46, aes_type %in% c("trend", "colorShapeEllipse")) %>% arrange(aes_type)

gridExtra::grid.arrange(
  suppressWarnings(gen.plot(lineup_dat, c("Color", "Shape"), c("Ellipses"))) +
    coord_fixed() + scale_x_continuous(limits = c(-3, 2.75)) + scale_y_continuous(limits = c(-3, 2.75)),

  suppressWarnings(gen.plot(lineup_dat, NULL, "Reg. Line")) +
    coord_fixed() + scale_x_continuous(limits = c(-3, 2.75)) + scale_y_continuous(limits = c(-3, 2.75)),
  nrow = 1
)
@

It is reasonable to think that $\hat\alpha$ would be different for the two lineups in \autoref{fig:featurehierarchy-null-model-sucks} even though the underlying data is the same: the color, shape, and ellipse aesthetics emphasize the missing elements and small cluster size, while this deficiency is much less apparent in the second lineup, which contains no aesthetic cues to indicate data clustering. We can estimate plot-specific $\hat\alpha$ for each of the plots shown in figure \autoref{fig:featurehierarchy-null-model-sucks}; the values are \Sexpr{sprintf("%0.3f and %0.3f", lineup_alphas$alpha[1], lineup_alphas$alpha[2])}, respectively. Thus, in the null plots shown in the first figure, which has color, shape, and ellipse aesthetics, $\theta_i, i = 1, ..., m_0$, the selection probabilities for each plot are more similar than in the second figure, which has only a trend line aesthetic.

We can examine the $\hat\alpha$ estimates for each set of aesthetics to examine this hypothesis. The full experiment details can be found in \citet{vanderplas:2017}; for the purposes of this reanalysis dealing only with the null plots, data was generated using three parameters: $k \in c(3, 5)$, the number of clusters, $\sigma_T$, the standard deviation around the trend line, and $\sigma_C$, the deviation around the cluster centers. Three values of $\sigma_T$ and $\sigma_C$ were used to represent varying degrees of difficulty, with small $\sigma$ values hypothesized to be easier than large $\sigma$ values. Thus, there are a total of 18 parameter combinations, and at each parameter combination there are 3 replicate lineup data sets, for a total of 54 datasets. Each dataset is rendered in 10 different aesthetic combinations, for a total of 540 plots.

With this design in mind, individual $\hat\alpha$ values were calculated from the null plot selection data for each lineup, to get a sense of the variability in the $\hat\alpha$ estimates. In some lineups, null plots were never selected; estimates have been excluded for these plots. \autoref{fig:all-null-alphas} shows the results by aesthetic for each lineup data set used in \citet{vanderplas:2017}.
<<all-null-alphas, fig.cap = "Alpha estimates for each lineup image, with aggregate estimates for each parameter set and aesthetic combination. There are some differences between aesthetics in average $\\hat\\alpha$, but overall, most $\\alpha$ values are estimated to be around 0.7. The variability in $\\hat\\alpha$ increases with the value of $\\sigma_C$ and $\\sigma_T$, the trend and cluster variability parameters. Note that on the y-axis, aesthetics are indicated as follows: C = color, L = ellipse, S = shape, T = trend line, E = error band. $\\hat\\alpha$s tend to be more variable when color aesthetics are present - if only one null plot has an unequal group, $\\alpha$ will tend to be very small; if more than one null plots have unequal groups, $\\alpha$ tends to be larger. Pooled estimates of the 3 plots generated with the same parameters and displayed with the same aesthetics are shown with an 'x'.", fig.width = 8, fig.height = 6>>=

t16_null_alpha_plotwise %>%
  mutate(valtype = "Individual Plot") %>%
  bind_rows(
    mutate(t16_null_alpha_setwise, valtype = "Pooled")
  ) %>%
  mutate(label = sprintf("Plots %02.0f-%02.0f", set_min, set_min+2),
         klab = sprintf("k == %d", k),
         siglab1 = paste("sigma[T] == ", round(sdline, 2)),
         siglab2 = paste("sigma[C] == ", round(sdgroup, 2)),
         aes_type = fix_plot_names(aes_type)) %>%
ggplot(aes(x = alpha, y = aes_type, size = n_total)) +
  scale_size_continuous("# Null Evaluations", range = c(1, 4)) +
  geom_point(aes(shape = valtype, alpha = valtype)) +
  scale_shape_manual("Estimate Type", values = c("Individual Plot" = 1, "Pooled" = 4)) +
  scale_alpha_manual("Estimate Type", values = c("Individual Plot" = .5, "Pooled" = 1)) +
  ylab("") +
  scale_x_continuous(name = expression(alpha)) +
  facet_grid(siglab1 ~klab + siglab2, labeller = label_parsed) +
  theme_bw() +
  theme(legend.position = "bottom", axis.title.y = element_blank())
@

<<all-null-alpha-pooled, fig.cap = "Alpha estimates calculated by pooling within null plot generating parameters and between null plot generating parameters. It is clear that the introduction of color and other aesthetics which highlight clustering causes an increase in $\\hat\\alpha$ estimates when data is aggregated; this effect is likely due to the inclusion of plots with several null targets with groups of noticably different sizes; there is significant variability in the $\\hat\\alpha$ estimates based on single data sets and based on single parameter sets because the randomly generated null plot data does not consistently produce the same number of null plots with this interesting characteristic.", fig.width = 4, fig.height = 5>>=
t16_null_alpha_setwise %>%
  mutate(valtype = "Pooled (single parameter set)") %>%
  bind_rows(
    mutate(t16_null_alpha_aeswise, valtype = "Pooled (all parameter sets)")
  ) %>%
  mutate(label = sprintf("Plots %02.0f-%02.0f", set_min, set_min+2),
         klab = sprintf("k == %d", k),
         siglab1 = paste("sigma[T] == ", round(sdline, 2)),
         siglab2 = paste("sigma[C] == ", round(sdgroup, 2)),
         aes_type = fix_plot_names(aes_type)) %>%
ggplot(aes(x = alpha, y = aes_type)) +
  geom_point(aes(shape = valtype, alpha = valtype)) +
  scale_shape_manual("Estimate Type", values = c("Pooled (single parameter set)" = 4, "Pooled (all parameter sets)" = 20)) +
  scale_alpha_manual("Estimate Type", values = c("Pooled (single parameter set)" = .5, "Pooled (all parameter sets)" = 1)) +
  ylab("") +
  scale_x_continuous(name = expression(alpha)) +
  theme_bw() +
  theme(legend.position = "bottom", axis.title.y = element_blank())
@

The estimated $\hat\alpha$ value can then be used either in the specification of the null model $\alpha$ ($\alpha_{M_2}$) or in the calculation of the visual p-value under the frequentist framework. Currently, in either scenario, the data used in the calculation are the target plot identifications and the sum of all null plot identifications; the individual values of the null plot identifications are not used. These data can be reclaimed to estimate a general $\alpha$ for the overall null model generation scheme, or, in an ideal situation, Rorshach lineups could be used to estimate $\alpha$ directly without any possible contamination effects induced by the presence of target plots.



\bibliographystyle{asa}
\bibliography{references}




\end{document}
